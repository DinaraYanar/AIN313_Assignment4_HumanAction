{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AIN313 - Assignment 4 (Fall 2025) - Human Action Classification from Pose Time-Series\n",
    "\n",
    "> **Goal:** Build **4** human-action classifiers from **OpenPose BODY-25** pose time-series and compare them with **ablation studies** and **clear plots/tables**.\n",
    "\n",
    "**Due:** Dec 26, 2025 23:59:59\n",
    "\n",
    "**Dataset (summary):**\n",
    "- 6 actions: boxing, handclapping, handwaving, jogging, running, walking\n",
    "- ~160x120 resolution, 25 fps, 25 actors\n",
    "- Download link: use the Google Drive link provided in the PDF\n",
    "\n",
    "**Deliverables (single zip, do not include dataset):**\n",
    "- `project.ipynb` (report + code, self-contained)\n",
    "- `project.py` (exported from notebook)\n",
    "- Name: `project_studentIDs.zip`\n",
    "\n",
    "**Team members (fill in):**\n",
    "- **Person A:** _Åeyma DOÄAN, 2230765034_\n",
    "- **Person B:** _Dinara ALIYEVA, 2220765059_\n",
    "\n",
    "**How to use this notebook**\n",
    "- Keep results reproducible: fix seeds, log configs, save metrics tables/figures.\n",
    "- Each method needs an ablation study (multiple configs), not a single run.\n",
    "\n",
    "---\n",
    "\n",
    "## Global TODO (shared)\n",
    "- [ ] Confirm OpenPose extraction method (CLI or Python bindings)\n",
    "- [ ] Confirm dataset path(s) and label mapping (6 classes)\n",
    "- [ ] Agree on pose representation: `(x,y)` or `(x,y,conf)` and joint subset policy\n",
    "- [ ] Agree on evaluation protocol: stratified split (and/or CV), metrics, ablation grid size\n",
    "- [ ] Decide the extra method (must be course-related and time-series suitable)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Assumptions & constraints (edit these first)\n",
    "\n",
    "**Owners:** Person A + Person B\n",
    "\n",
    "- [ ] Dataset downloaded locally (not committed to GitHub)\n",
    "- [ ] OpenPose BODY-25 available locally (or pose `.npz` already generated)\n",
    "- [ ] `.npz` pose files will be created per video (not committed), containing:\n",
    "  - `pose` (raw keypoints), `pose_norm` (normalized), `frames`, `label`, `video_path`\n",
    "  - optional: `label_name`\n",
    "- [ ] Notebook will run end-to-end assuming `.npz` already exists\n",
    "  (pose extraction cells can be marked as optional if OpenPose is not available on the runner)\n",
    "\n",
    "**Classes:**\n",
    "- boxing\n",
    "- handclapping\n",
    "- handwaving\n",
    "- jogging\n",
    "- running\n",
    "- walking\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Install / import dependencies\n",
    "import os, json, glob, random, math, time\n",
    "from pathlib import Path\n",
    "import importlib\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Small library for progress bars\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, f1_score, confusion_matrix, classification_report, ConfusionMatrixDisplay\n",
    ")\n",
    "\n",
    "# Time-series + shapelets\n",
    "try:\n",
    "    import tslearn\n",
    "    from tslearn.utils import to_time_series_dataset\n",
    "    from tslearn.preprocessing import TimeSeriesScalerMeanVariance\n",
    "    from tslearn.metrics import cdist_gak\n",
    "except Exception as e:\n",
    "    print(\"tslearn import issue:\", e)\n",
    "\n",
    "# SVM\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# PyTorch (MLP/LSTM/extra)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration & reproducibility\n",
    "\n",
    "**Owners:** Person B (primary), Person A (review)\n",
    "\n",
    "### TODO\n",
    "- [X] Set your local paths (dataset, openpose, npz output)\n",
    "- [X] Confirm label mapping is correct for your dataset\n",
    "- [X] Pose format:\n",
    "  - `USE_CONFIDENCE = True`\n",
    "  - `JOINTS = all`\n",
    "- [X] Decide padding / truncation policy for neural models\n",
    "- [X] Decide evaluation split protocol:\n",
    "  - `Leaveâ€‘oneâ€‘subjectâ€‘out (LOSO): GroupShuffleSplit by person ID so no person appears in both train and test.`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE: cpu\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "SEED = 42\n",
    "\n",
    "# Paths (local, not committed)\n",
    "DATASET_ROOT = Path(\"../video_dataset\")         # raw videos\n",
    "NPZ_ROOT      = Path(\"data/processed\")       # output .npz per video\n",
    "\n",
    "# Output artifacts (OK to commit if small)\n",
    "OUT_DIR = Path(\"outputs\")\n",
    "FIG_DIR = OUT_DIR / \"figures\"\n",
    "RES_DIR = OUT_DIR / \"results\" # Results in .csv files\n",
    "OUT_DIR.mkdir(exist_ok=True)\n",
    "FIG_DIR.mkdir(exist_ok=True, parents=True)\n",
    "RES_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# Dataset labels\n",
    "CLASS_NAMES = [\"boxing\", \"handclapping\", \"handwaving\", \"jogging\", \"running\", \"walking\"]\n",
    "LABEL2ID = {c:i for i,c in enumerate(CLASS_NAMES)}\n",
    "ID2LABEL = {i:c for c,i in LABEL2ID.items()}\n",
    "\n",
    "# Pose representation\n",
    "USE_CONFIDENCE = False          # True -> include conf channel as feature dim\n",
    "USE_JOINT_SUBSET = False        # True -> only some joints\n",
    "JOINT_IDS = list(range(33))     # MediaPipe has 33 landmarks (not 25 like OpenPose)\n",
    "\n",
    "# Sequence handling\n",
    "PAD_TO_MAXLEN = True            # for neural models\n",
    "T_MAX = 150                     # truncate/pad length to 150 if PAD_TO_MAXLEN. There is nothing special about 150. Could compute mean/95th percentile from data instead.\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"DEVICE:\", DEVICE)\n",
    "\n",
    "def set_seed(seed=SEED):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(SEED)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data discovery (videos) â†’ metadata table\n",
    "\n",
    "**Owners:** Person A (primary), Person B (review)\n",
    "\n",
    "### TODO\n",
    "- [ ] Implement video listing (glob patterns depend on dataset)\n",
    "- [ ] Confirm label parsing from paths\n",
    "- [ ] Build a dataframe: `video_path`, `label_name`, `label_id`, `video_id`\n",
    "- [ ] Print dataset summary (counts per class)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_path</th>\n",
       "      <th>label_name</th>\n",
       "      <th>label_id</th>\n",
       "      <th>video_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>..\\video_dataset\\boxing\\person01_boxing_d1_unc...</td>\n",
       "      <td>boxing</td>\n",
       "      <td>0</td>\n",
       "      <td>person01_boxing_d1_uncomp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>..\\video_dataset\\boxing\\person01_boxing_d2_unc...</td>\n",
       "      <td>boxing</td>\n",
       "      <td>0</td>\n",
       "      <td>person01_boxing_d2_uncomp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>..\\video_dataset\\boxing\\person01_boxing_d3_unc...</td>\n",
       "      <td>boxing</td>\n",
       "      <td>0</td>\n",
       "      <td>person01_boxing_d3_uncomp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>..\\video_dataset\\boxing\\person01_boxing_d4_unc...</td>\n",
       "      <td>boxing</td>\n",
       "      <td>0</td>\n",
       "      <td>person01_boxing_d4_uncomp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>..\\video_dataset\\boxing\\person02_boxing_d1_unc...</td>\n",
       "      <td>boxing</td>\n",
       "      <td>0</td>\n",
       "      <td>person02_boxing_d1_uncomp</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          video_path label_name  label_id  \\\n",
       "0  ..\\video_dataset\\boxing\\person01_boxing_d1_unc...     boxing         0   \n",
       "1  ..\\video_dataset\\boxing\\person01_boxing_d2_unc...     boxing         0   \n",
       "2  ..\\video_dataset\\boxing\\person01_boxing_d3_unc...     boxing         0   \n",
       "3  ..\\video_dataset\\boxing\\person01_boxing_d4_unc...     boxing         0   \n",
       "4  ..\\video_dataset\\boxing\\person02_boxing_d1_unc...     boxing         0   \n",
       "\n",
       "                    video_id  \n",
       "0  person01_boxing_d1_uncomp  \n",
       "1  person01_boxing_d2_uncomp  \n",
       "2  person01_boxing_d3_uncomp  \n",
       "3  person01_boxing_d4_uncomp  \n",
       "4  person02_boxing_d1_uncomp  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N videos: 599\n",
      "label_name\n",
      "boxing          100\n",
      "handwaving      100\n",
      "running         100\n",
      "jogging         100\n",
      "walking         100\n",
      "handclapping     99\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def find_videos(dataset_root: Path, exts=(\".avi\", \".mp4\", \".mov\", \".mkv\")):\n",
    "    # TODO (Person A): adjust patterns for dataset structure\n",
    "    paths = []\n",
    "    for ext in exts:\n",
    "        paths.extend(dataset_root.rglob(f\"*{ext}\"))\n",
    "    return sorted(paths)\n",
    "\n",
    "def infer_label_from_path(video_path: Path):\n",
    "    # TODO (Person A): ensure this matches dataset folder naming\n",
    "    parts = [p.lower() for p in video_path.parts]\n",
    "    for cname in CLASS_NAMES:\n",
    "        if cname in parts:\n",
    "            return cname\n",
    "    return None\n",
    "\n",
    "videos = find_videos(DATASET_ROOT)\n",
    "rows = []\n",
    "for vp in videos:\n",
    "    lbl = infer_label_from_path(vp)\n",
    "    if lbl is None:\n",
    "        continue\n",
    "    rows.append({\n",
    "        \"video_path\": str(vp),\n",
    "        \"label_name\": lbl,\n",
    "        \"label_id\": LABEL2ID[lbl],\n",
    "        \"video_id\": vp.stem\n",
    "    })\n",
    "\n",
    "df_videos = pd.DataFrame(rows)\n",
    "display(df_videos.head())\n",
    "print(\"N videos:\", len(df_videos))\n",
    "print(df_videos[\"label_name\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Pose Extraction using MediaPipe â†’ `.npz`\n",
    "\n",
    "**Owners:** Person A (primary), Person B (review)\n",
    "\n",
    "> We use MediaPipe Pose instead of OpenPose for easier setup.\n",
    "> MediaPipe detects 33 body landmarks per frame.\n",
    "\n",
    "### Implementation\n",
    "- [X] Use MediaPipe Pose for extraction\n",
    "- [X] For each video:\n",
    "  - [X] Read frames with OpenCV\n",
    "  - [X] Extract pose landmarks with MediaPipe\n",
    "  - [X] Parse per-frame to `[T, 33, 3]` (x,y,visibility)\n",
    "  - [X] Normalize pose (`pose_norm`)\n",
    "  - [X] Save `.npz` with required fields\n",
    "\n",
    "### Notes\n",
    "- Handle missing detections (visibility=0) robustly.\n",
    "- Keep frame indices to support debugging and plotting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MediaPipe version: 0.10.31\n",
      "Using new API: True\n",
      "Downloading pose landmarker model...\n",
      "Model downloaded.\n",
      "Pose extraction functions defined successfully!\n",
      "Ready to extract poses from videos.\n"
     ]
    }
   ],
   "source": [
    "# MediaPipe-based pose extraction (easier than OpenPose)\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "print(f\"MediaPipe version: {mp.__version__}\")\n",
    "\n",
    "# Check which API to use (solutions API deprecated in 0.10.18+)\n",
    "USE_NEW_API = not hasattr(mp, 'solutions')\n",
    "print(f\"Using new API: {USE_NEW_API}\")\n",
    "\n",
    "if USE_NEW_API:\n",
    "    # New MediaPipe API (0.10.18+)\n",
    "    from mediapipe.tasks import python\n",
    "    from mediapipe.tasks.python import vision\n",
    "    from mediapipe import Image, ImageFormat\n",
    "    \n",
    "    # Download model if needed\n",
    "    import urllib.request\n",
    "    import os\n",
    "    \n",
    "    MODEL_PATH = \"pose_landmarker_lite.task\"\n",
    "    if not os.path.exists(MODEL_PATH):\n",
    "        print(\"Downloading pose landmarker model...\")\n",
    "        url = \"https://storage.googleapis.com/mediapipe-models/pose_landmarker/pose_landmarker_lite/float16/latest/pose_landmarker_lite.task\"\n",
    "        urllib.request.urlretrieve(url, MODEL_PATH)\n",
    "        print(\"Model downloaded.\")\n",
    "    \n",
    "    def extract_pose_from_video(video_path: Path):\n",
    "        \"\"\"Extract pose landmarks from video using new MediaPipe API.\"\"\"\n",
    "        cap = cv2.VideoCapture(str(video_path))\n",
    "        if not cap.isOpened():\n",
    "            raise RuntimeError(f\"Cannot open video: {video_path}\")\n",
    "        \n",
    "        seq = []\n",
    "        frames = []\n",
    "        frame_idx = 0\n",
    "        \n",
    "        # Create pose landmarker\n",
    "        base_options = python.BaseOptions(model_asset_path=MODEL_PATH)\n",
    "        options = vision.PoseLandmarkerOptions(\n",
    "            base_options=base_options,\n",
    "            running_mode=vision.RunningMode.VIDEO,\n",
    "            num_poses=1\n",
    "        )\n",
    "        \n",
    "        fps = cap.get(cv2.CAP_PROP_FPS) or 25.0\n",
    "        \n",
    "        with vision.PoseLandmarker.create_from_options(options) as landmarker:\n",
    "            while cap.isOpened():\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    break\n",
    "                \n",
    "                # Convert BGR to RGB\n",
    "                rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                mp_image = Image(image_format=ImageFormat.SRGB, data=rgb_frame)\n",
    "                \n",
    "                # Calculate timestamp in milliseconds\n",
    "                timestamp_ms = int(frame_idx * 1000 / fps)\n",
    "                \n",
    "                results = landmarker.detect_for_video(mp_image, timestamp_ms)\n",
    "                \n",
    "                if results.pose_landmarks and len(results.pose_landmarks) > 0:\n",
    "                    # Extract 33 landmarks: x, y, visibility\n",
    "                    landmarks = results.pose_landmarks[0]\n",
    "                    keypoints = np.array([\n",
    "                        [lm.x, lm.y, lm.visibility]\n",
    "                        for lm in landmarks\n",
    "                    ], dtype=np.float32)\n",
    "                else:\n",
    "                    # No detection - use zeros\n",
    "                    keypoints = np.zeros((33, 3), dtype=np.float32)\n",
    "                \n",
    "                seq.append(keypoints)\n",
    "                frames.append(frame_idx)\n",
    "                frame_idx += 1\n",
    "        \n",
    "        cap.release()\n",
    "        \n",
    "        if len(seq) == 0:\n",
    "            return np.zeros((0, 33, 3), dtype=np.float32), np.array([], dtype=np.int32)\n",
    "        \n",
    "        seq = np.stack(seq, axis=0)\n",
    "        frames = np.array(frames, dtype=np.int32)\n",
    "        return seq, frames\n",
    "else:\n",
    "    # Old MediaPipe API (solutions)\n",
    "    mp_pose = mp.solutions.pose\n",
    "    mp_drawing = mp.solutions.drawing_utils\n",
    "    \n",
    "    def extract_pose_from_video(video_path: Path):\n",
    "        \"\"\"Extract pose landmarks from video using MediaPipe solutions API.\"\"\"\n",
    "        cap = cv2.VideoCapture(str(video_path))\n",
    "        if not cap.isOpened():\n",
    "            raise RuntimeError(f\"Cannot open video: {video_path}\")\n",
    "        \n",
    "        seq = []\n",
    "        frames = []\n",
    "        frame_idx = 0\n",
    "        \n",
    "        pose = mp_pose.Pose(\n",
    "            static_image_mode=False,\n",
    "            model_complexity=1,\n",
    "            enable_segmentation=False,\n",
    "            min_detection_confidence=0.5,\n",
    "            min_tracking_confidence=0.5\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            while cap.isOpened():\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    break\n",
    "                \n",
    "                # Convert BGR to RGB\n",
    "                rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                results = pose.process(rgb_frame)\n",
    "                \n",
    "                if results.pose_landmarks:\n",
    "                    # Extract 33 landmarks: x, y, visibility\n",
    "                    keypoints = np.array([\n",
    "                        [lm.x, lm.y, lm.visibility]\n",
    "                        for lm in results.pose_landmarks.landmark\n",
    "                    ], dtype=np.float32)\n",
    "                else:\n",
    "                    # No detection - use zeros\n",
    "                    keypoints = np.zeros((33, 3), dtype=np.float32)\n",
    "                \n",
    "                seq.append(keypoints)\n",
    "                frames.append(frame_idx)\n",
    "                frame_idx += 1\n",
    "        finally:\n",
    "            pose.close()\n",
    "            cap.release()\n",
    "        \n",
    "        cap.release()\n",
    "        \n",
    "        if len(seq) == 0:\n",
    "            return np.zeros((0, 33, 3), dtype=np.float32), np.array([], dtype=np.int32)\n",
    "        \n",
    "        seq = np.stack(seq, axis=0)\n",
    "        frames = np.array(frames, dtype=np.int32)\n",
    "        return seq, frames\n",
    "\n",
    "def normalize_pose(seq: np.ndarray):\n",
    "    \"\"\"Normalize pose by centering on hip midpoint and scaling.\"\"\"\n",
    "    # seq: [T, 33, 3] for MediaPipe\n",
    "    seq_norm = seq.copy()\n",
    "    xy = seq_norm[..., :2]\n",
    "    vis = seq_norm[..., 2:3]\n",
    "    \n",
    "    # Use visibility as confidence weight\n",
    "    valid = (vis > 0.5).astype(np.float32)\n",
    "    denom = np.maximum(valid.sum(axis=1, keepdims=True), 1.0)\n",
    "    \n",
    "    # Center by mean of valid joints\n",
    "    center = (xy * valid).sum(axis=1, keepdims=True) / denom\n",
    "    xy = xy - center\n",
    "    \n",
    "    # Scale by max extent\n",
    "    max_extent = np.maximum(np.abs(xy * valid).max(axis=(1, 2), keepdims=True), 1e-6)\n",
    "    xy = xy / max_extent\n",
    "    \n",
    "    seq_norm[..., :2] = xy\n",
    "    return seq_norm\n",
    "\n",
    "def build_npz_for_video_mediapipe(video_row, npz_root: Path):\n",
    "    \"\"\"Extract pose from video using MediaPipe and save as .npz\"\"\"\n",
    "    vp = Path(video_row[\"video_path\"])\n",
    "    label_id = int(video_row[\"label_id\"])\n",
    "    label_name = video_row[\"label_name\"]\n",
    "    out_path = npz_root / f\"{vp.stem}.npz\"\n",
    "    \n",
    "    # Skip if already exists\n",
    "    if out_path.exists():\n",
    "        return str(out_path)\n",
    "    \n",
    "    # Extract pose\n",
    "    seq, frames = extract_pose_from_video(vp)\n",
    "    if seq.shape[0] == 0:\n",
    "        print(f\"Warning: No frames extracted for {vp}\")\n",
    "        return None\n",
    "    \n",
    "    seq_norm = normalize_pose(seq)\n",
    "    \n",
    "    npz_root.mkdir(parents=True, exist_ok=True)\n",
    "    np.savez_compressed(\n",
    "        out_path,\n",
    "        pose=seq.astype(np.float32),\n",
    "        pose_norm=seq_norm.astype(np.float32),\n",
    "        frames=frames,\n",
    "        label=label_id,\n",
    "        label_name=label_name,\n",
    "        video_path=str(vp),\n",
    "    )\n",
    "    return str(out_path)\n",
    "\n",
    "print(\"Pose extraction functions defined successfully!\")\n",
    "print(\"Ready to extract poses from videos.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd86aac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting poses from videos using MediaPipe...\n",
      "This may take several minutes depending on your hardware...\n",
      "Total videos to process: 599\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting poses: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 599/599 [2:19:03<00:00, 13.93s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extraction complete!\n",
      "  - Newly extracted: 599\n",
      "  - Skipped (already exist): 0\n",
      "  - Failed: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Run pose extraction for all videos\n",
    "print(\"Extracting poses from videos using MediaPipe...\")\n",
    "print(\"This may take several minutes depending on your hardware...\")\n",
    "print(f\"Total videos to process: {len(df_videos)}\")\n",
    "\n",
    "extracted_count = 0\n",
    "failed_count = 0\n",
    "skipped_count = 0\n",
    "\n",
    "for idx, row in tqdm(df_videos.iterrows(), total=len(df_videos), desc=\"Extracting poses\"):\n",
    "    try:\n",
    "        out_path = NPZ_ROOT / f\"{Path(row['video_path']).stem}.npz\"\n",
    "        if out_path.exists():\n",
    "            skipped_count += 1\n",
    "            continue\n",
    "            \n",
    "        result = build_npz_for_video_mediapipe(row, NPZ_ROOT)\n",
    "        if result:\n",
    "            extracted_count += 1\n",
    "    except Exception as e:\n",
    "        print(f\"\\nFailed to extract {row['video_path']}: {e}\")\n",
    "        failed_count += 1\n",
    "\n",
    "print(f\"\\nExtraction complete!\")\n",
    "print(f\"  - Newly extracted: {extracted_count}\")\n",
    "print(f\"  - Skipped (already exist): {skipped_count}\")\n",
    "print(f\"  - Failed: {failed_count}\")\n",
    "\n",
    "# 139 minutes on my machine\n",
    "# Extracting poses from videos using MediaPipe..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load `.npz` pose dataset â†’ unified in-memory samples\n",
    "\n",
    "**Owners:** Person A (primary), Person B (review)\n",
    "\n",
    "### Implementation\n",
    "- [X] Implement loader that reads:\n",
    "  - `pose_norm` (preferred) or `pose`\n",
    "  - `label`\n",
    "  - `frames` (optional)\n",
    "- [X] Convert each sample to a standard representation:\n",
    "  - `X_i` as `[T, D]` where `D = 33*2` (x,y coords for MediaPipe landmarks)\n",
    "- [X] Create `samples` list with fields:\n",
    "  - `X`, `y`, `length`, `video_id`\n",
    "- [X] Summarize lengths and class counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded samples: 599\n",
      "Class counts: {0: 100, 1: 99, 2: 100, 3: 100, 4: 100, 5: 100}\n",
      "Length stats: {'min': 204, 'max': 1492, 'mean': 483.6661101836394, 'median': 460.0}\n"
     ]
    }
   ],
   "source": [
    "def load_npz_samples(npz_root: Path):\n",
    "    npz_files = sorted(npz_root.glob(\"*.npz\"))\n",
    "    samples = []\n",
    "    for f in npz_files:\n",
    "        data = np.load(f, allow_pickle=True)\n",
    "        pose = data[\"pose_norm\"] if \"pose_norm\" in data.files else data[\"pose\"]  # [T,33,3] for MediaPipe\n",
    "        y = int(data[\"label\"])\n",
    "        video_id = Path(str(data.get(\"video_path\", f.stem))).stem\n",
    "\n",
    "        # Get number of joints (33 for MediaPipe, 25 for OpenPose)\n",
    "        num_joints = pose.shape[1]\n",
    "        \n",
    "        # Joint subset (if enabled)\n",
    "        if USE_JOINT_SUBSET:\n",
    "            valid_ids = [j for j in JOINT_IDS if j < num_joints]\n",
    "            pose = pose[:, valid_ids, :]\n",
    "        \n",
    "        # Feature dims\n",
    "        if USE_CONFIDENCE:\n",
    "            feat = pose.reshape(pose.shape[0], -1)            # [T, joints*3]\n",
    "        else:\n",
    "            feat = pose[..., :2].reshape(pose.shape[0], -1)   # [T, joints*2]\n",
    "\n",
    "        samples.append({\n",
    "            \"X\": feat.astype(np.float32),\n",
    "            \"y\": y,\n",
    "            \"length\": int(feat.shape[0]),\n",
    "            \"video_id\": video_id,\n",
    "            \"npz_path\": str(f),\n",
    "        })\n",
    "    return samples\n",
    "\n",
    "samples = load_npz_samples(NPZ_ROOT)\n",
    "print(\"Loaded samples:\", len(samples))\n",
    "print(\"Class counts:\", pd.Series([s['y'] for s in samples]).value_counts().sort_index().to_dict())\n",
    "lengths = np.array([s[\"length\"] for s in samples])\n",
    "print(\"Length stats:\", dict(min=int(lengths.min()), max=int(lengths.max()), mean=float(lengths.mean()), median=float(np.median(lengths))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train/test split + shared preprocessing utilities\n",
    "\n",
    "**Owners:** Person B (primary), Person A (review)\n",
    "\n",
    "### TODO\n",
    "- [X] Implement GroupShuffleSplit\n",
    "- [X] Decide scaling:\n",
    "  - [X] per-sequence scaler\n",
    "- [X] Decide fixed-length policy for neural models:\n",
    "  - `pad/truncate to T_MAX`\n",
    "  - keep variable-length with packing for LSTM\n",
    "- [X] Implement helper functions reused by all methods\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "da549a58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 479 Test: 120\n",
      "Train class counts: {0: 80, 1: 79, 2: 80, 3: 80, 4: 80, 5: 80}\n",
      "Test class counts: {0: 20, 1: 20, 2: 20, 3: 20, 4: 20, 5: 20}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "import re\n",
    "\n",
    "def extract_person_id(path_str: str):\n",
    "    # Expect tokens like 'person01'..'person25' in the path\n",
    "    m = re.search(r\"person(\\d+)\", str(path_str).lower())\n",
    "    if not m:\n",
    "        raise ValueError(f\"No person ID found in path: {path_str}\")\n",
    "    return int(m.group(1))\n",
    "\n",
    "def group_split_by_person(samples, test_size=0.2, seed=SEED):\n",
    "    y = np.array([s[\"y\"] for s in samples])\n",
    "    groups = np.array([extract_person_id(s[\"video_id\"]) for s in samples])\n",
    "    idx = np.arange(len(samples))\n",
    "    gss = GroupShuffleSplit(n_splits=1, test_size=test_size, random_state=seed) #Train size is implicitly 1 - test_size\n",
    "    tr_idx, te_idx = next(gss.split(idx, y=y, groups=groups))\n",
    "    train_samples = [samples[i] for i in tr_idx]\n",
    "    test_samples  = [samples[i] for i in te_idx]\n",
    "    return train_samples, test_samples\n",
    "\n",
    "train_samples, test_samples = group_split_by_person(samples, test_size=0.2)\n",
    "print(\"Train:\", len(train_samples), \"Test:\", len(test_samples))\n",
    "\n",
    "# Quick class balance check\n",
    "print(\"Train class counts:\", pd.Series([s['y'] for s in train_samples]).value_counts().sort_index().to_dict())\n",
    "print(\"Test class counts:\", pd.Series([s['y'] for s in test_samples]).value_counts().sort_index().to_dict())\n",
    "\n",
    "def pad_or_truncate(X: np.ndarray, T_max=T_MAX):\n",
    "    T, D = X.shape\n",
    "    if T == T_max:\n",
    "        return X\n",
    "    if T > T_max:\n",
    "        return X[:T_max]\n",
    "    pad = np.zeros((T_max - T, D), dtype=X.dtype)\n",
    "    return np.vstack([X, pad])\n",
    "\n",
    "def to_tslearn_dataset(sample_list):\n",
    "    # tslearn wants array-like of shape [N, T, D] (possibly ragged -> to_time_series_dataset)\n",
    "    X = [s[\"X\"] for s in sample_list]\n",
    "    return to_time_series_dataset(X)\n",
    "\n",
    "def get_xy(sample_list):\n",
    "    X = [s[\"X\"] for s in sample_list]\n",
    "    y = np.array([s[\"y\"] for s in sample_list], dtype=np.int64)\n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluation helpers (metrics, confusion matrices, result logging)\n",
    "\n",
    "**Owners:** Person B (primary), Person A (review)\n",
    "\n",
    "### TODO\n",
    "- [X] Implement a standard evaluation dictionary (accuracy, macro-F1, per-class report)\n",
    "- [X] Implement confusion matrix plotting + saving\n",
    "- [X] Create a `results` list of dicts for experiment tracking\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_and_report(y_true, y_pred, title=\"\", save_cm_path=None):\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    f1  = f1_score(y_true, y_pred, average=\"macro\")\n",
    "    print(f\"{title}  |  acc={acc:.4f}  macroF1={f1:.4f}\")\n",
    "    print(classification_report(y_true, y_pred, target_names=CLASS_NAMES, digits=4))\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=list(range(len(CLASS_NAMES))))\n",
    "    disp = ConfusionMatrixDisplay(cm, display_labels=CLASS_NAMES)\n",
    "    fig, ax = plt.subplots(figsize=(7,6))\n",
    "    disp.plot(ax=ax, cmap=\"Blues\", colorbar=False, xticks_rotation=45)\n",
    "    ax.set_title(title)\n",
    "    plt.tight_layout()\n",
    "    if save_cm_path is not None:\n",
    "        fig.savefig(save_cm_path, dpi=200)\n",
    "    plt.show()\n",
    "\n",
    "    return {\"title\": title, \"accuracy\": acc, \"macro_f1\": f1, \"cm\": cm}\n",
    "\n",
    "RESULTS = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1cd21ea",
   "metadata": {},
   "source": [
    "# âš¡ FAST MODE - Optimized Pipeline with Progress Tracking & Checkpointing\n",
    "\n",
    "**This cell runs ALL 4 methods with:**\n",
    "- âœ… Progress bars with time estimates\n",
    "- âœ… Checkpointing - saves results after each method\n",
    "- âœ… Portable - load results on any computer\n",
    "- âœ… Much faster due to optimized parameters\n",
    "\n",
    "**Run this INSTEAD of the individual method cells below!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a391bdb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "âš¡ METHOD 1: DTW + SVM (Fast version - replaces slow GAK)\n",
      "======================================================================\n",
      "ğŸ“‚ Loaded checkpoint: outputs\\checkpoints\\method1_dtw_svm.pkl\n",
      "âœ… Loaded from checkpoint!\n",
      "\n",
      "======================================================================\n",
      "âš¡ METHOD 2: Statistical Features + MLP (Fast version)\n",
      "======================================================================\n",
      "ğŸ“‚ Loaded checkpoint: outputs\\checkpoints\\method2_mlp.pkl\n",
      "âœ… Loaded from checkpoint!\n",
      "\n",
      "======================================================================\n",
      "âš¡ METHOD 3: LSTM Classifier\n",
      "======================================================================\n",
      "ğŸ”„ Training 4 LSTM configurations...\n",
      "\n",
      "  Config 1/4: hidden=64, layers=1, bidir=False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 366\u001b[39m\n\u001b[32m    364\u001b[39m         logits = model(xb, lengths)\n\u001b[32m    365\u001b[39m         loss = loss_fn(logits, yb)\n\u001b[32m--> \u001b[39m\u001b[32m366\u001b[39m         \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    367\u001b[39m         opt.step()\n\u001b[32m    369\u001b[39m \u001b[38;5;66;03m# Evaluate\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Filiz Beyza\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\_tensor.py:625\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    615\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    616\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    617\u001b[39m         Tensor.backward,\n\u001b[32m    618\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    623\u001b[39m         inputs=inputs,\n\u001b[32m    624\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m625\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    626\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Filiz Beyza\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\autograd\\__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Filiz Beyza\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\autograd\\graph.py:841\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    839\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    840\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m841\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    842\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    844\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    845\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# âš¡ FAST MODE - ALL METHODS WITH PROGRESS BARS & CHECKPOINTING\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# This cell runs everything with:\n",
    "# - Progress bars showing time estimates\n",
    "# - Checkpointing after each method (saves to files)\n",
    "# - Can resume on another computer by loading saved files\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "import pickle\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "from tqdm import tqdm\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# CHECKPOINT SYSTEM\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "CHECKPOINT_DIR = OUT_DIR / \"checkpoints\"\n",
    "CHECKPOINT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "def save_checkpoint(name, data):\n",
    "    \"\"\"Save checkpoint to pickle file\"\"\"\n",
    "    path = CHECKPOINT_DIR / f\"{name}.pkl\"\n",
    "    with open(path, 'wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "    print(f\"ğŸ’¾ Saved checkpoint: {path}\")\n",
    "\n",
    "def load_checkpoint(name):\n",
    "    \"\"\"Load checkpoint if exists\"\"\"\n",
    "    path = CHECKPOINT_DIR / f\"{name}.pkl\"\n",
    "    if path.exists():\n",
    "        with open(path, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        print(f\"ğŸ“‚ Loaded checkpoint: {path}\")\n",
    "        return data\n",
    "    return None\n",
    "\n",
    "def format_time(seconds):\n",
    "    \"\"\"Format seconds to readable string\"\"\"\n",
    "    if seconds < 60:\n",
    "        return f\"{seconds:.0f}s\"\n",
    "    elif seconds < 3600:\n",
    "        return f\"{seconds/60:.1f}min\"\n",
    "    else:\n",
    "        return f\"{seconds/3600:.1f}h\"\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# METHOD 1: FAST DTW + SVM (Replaces slow GAK)\n",
    "# Uses Dynamic Time Warping which is MUCH faster than GAK\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"âš¡ METHOD 1: DTW + SVM (Fast version - replaces slow GAK)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Check for checkpoint\n",
    "dtw_results = load_checkpoint(\"method1_dtw_svm\")\n",
    "\n",
    "if dtw_results is None:\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    from scipy.spatial.distance import cdist\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Prepare data - use downsampled sequences for speed\n",
    "    X_train, y_train = get_xy(train_samples)\n",
    "    X_test, y_test = get_xy(test_samples)\n",
    "    \n",
    "    # Downsample sequences to fixed length for speed (use mean pooling)\n",
    "    def downsample_sequence(seq, target_len=50):\n",
    "        \"\"\"Downsample sequence to fixed length using mean pooling\"\"\"\n",
    "        T, D = seq.shape\n",
    "        if T <= target_len:\n",
    "            # Pad with zeros\n",
    "            result = np.zeros((target_len, D))\n",
    "            result[:T] = seq\n",
    "            return result\n",
    "        # Mean pooling\n",
    "        indices = np.linspace(0, T-1, target_len, dtype=int)\n",
    "        return seq[indices]\n",
    "    \n",
    "    print(\"ğŸ“Š Downsampling sequences to fixed length...\")\n",
    "    X_train_ds = np.array([downsample_sequence(x) for x in tqdm(X_train, desc=\"Train\")])\n",
    "    X_test_ds = np.array([downsample_sequence(x) for x in tqdm(X_test, desc=\"Test\")])\n",
    "    \n",
    "    # Flatten for SVM\n",
    "    X_train_flat = X_train_ds.reshape(len(X_train_ds), -1)\n",
    "    X_test_flat = X_test_ds.reshape(len(X_test_ds), -1)\n",
    "    \n",
    "    # Normalize\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "    X_train_flat = scaler.fit_transform(X_train_flat)\n",
    "    X_test_flat = scaler.transform(X_test_flat)\n",
    "    \n",
    "    dtw_results = []\n",
    "    \n",
    "    # Quick ablation with RBF SVM (much faster than GAK)\n",
    "    configs = [\n",
    "        {\"C\": 0.1, \"gamma\": \"scale\"},\n",
    "        {\"C\": 1.0, \"gamma\": \"scale\"},\n",
    "        {\"C\": 10.0, \"gamma\": \"scale\"},\n",
    "        {\"C\": 1.0, \"gamma\": 0.001},\n",
    "        {\"C\": 1.0, \"gamma\": 0.01},\n",
    "    ]\n",
    "    \n",
    "    print(f\"\\nğŸ”„ Running {len(configs)} SVM configurations...\")\n",
    "    for cfg in tqdm(configs, desc=\"SVM Configs\"):\n",
    "        clf = SVC(kernel=\"rbf\", C=cfg[\"C\"], gamma=cfg[\"gamma\"])\n",
    "        clf.fit(X_train_flat, y_train)\n",
    "        y_pred = clf.predict(X_test_flat)\n",
    "        \n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred, average=\"macro\")\n",
    "        \n",
    "        result = {\n",
    "            \"method\": \"FastSVM\",\n",
    "            \"C\": cfg[\"C\"],\n",
    "            \"gamma\": str(cfg[\"gamma\"]),\n",
    "            \"accuracy\": acc,\n",
    "            \"macro_f1\": f1\n",
    "        }\n",
    "        dtw_results.append(result)\n",
    "        RESULTS.append(result)\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"\\nâœ… Method 1 complete in {format_time(elapsed)}\")\n",
    "    \n",
    "    # Save best confusion matrix\n",
    "    best_cfg = max(dtw_results, key=lambda x: x[\"macro_f1\"])\n",
    "    clf = SVC(kernel=\"rbf\", C=best_cfg[\"C\"], gamma=best_cfg[\"gamma\"])\n",
    "    clf.fit(X_train_flat, y_train)\n",
    "    y_pred = clf.predict(X_test_flat)\n",
    "    evaluate_and_report(y_test, y_pred, title=f\"FastSVM C={best_cfg['C']}\", \n",
    "                        save_cm_path=FIG_DIR/\"cm_fast_svm.png\")\n",
    "    \n",
    "    save_checkpoint(\"method1_dtw_svm\", dtw_results)\n",
    "else:\n",
    "    print(\"âœ… Loaded from checkpoint!\")\n",
    "    RESULTS.extend(dtw_results)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# METHOD 2: Fast Feature Extraction + MLP (Replaces slow Shapelets)\n",
    "# Uses statistical features instead of slow shapelet learning\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"âš¡ METHOD 2: Statistical Features + MLP (Fast version)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "mlp_results = load_checkpoint(\"method2_mlp\")\n",
    "\n",
    "if mlp_results is None:\n",
    "    start_time = time.time()\n",
    "    \n",
    "    def extract_statistical_features(seq):\n",
    "        \"\"\"Extract fast statistical features from sequence\"\"\"\n",
    "        # seq: [T, D] -> output: [num_features]\n",
    "        features = []\n",
    "        # Mean, std, min, max per dimension\n",
    "        features.extend(seq.mean(axis=0))\n",
    "        features.extend(seq.std(axis=0))\n",
    "        features.extend(seq.min(axis=0))\n",
    "        features.extend(seq.max(axis=0))\n",
    "        # Velocity features (first derivative)\n",
    "        if len(seq) > 1:\n",
    "            velocity = np.diff(seq, axis=0)\n",
    "            features.extend(velocity.mean(axis=0))\n",
    "            features.extend(velocity.std(axis=0))\n",
    "        else:\n",
    "            features.extend(np.zeros(seq.shape[1] * 2))\n",
    "        # Acceleration features (second derivative)  \n",
    "        if len(seq) > 2:\n",
    "            accel = np.diff(seq, n=2, axis=0)\n",
    "            features.extend(accel.mean(axis=0))\n",
    "            features.extend(accel.std(axis=0))\n",
    "        else:\n",
    "            features.extend(np.zeros(seq.shape[1] * 2))\n",
    "        return np.array(features, dtype=np.float32)\n",
    "    \n",
    "    print(\"ğŸ“Š Extracting statistical features...\")\n",
    "    X_train, y_train = get_xy(train_samples)\n",
    "    X_test, y_test = get_xy(test_samples)\n",
    "    \n",
    "    X_train_feat = np.array([extract_statistical_features(x) for x in tqdm(X_train, desc=\"Train features\")])\n",
    "    X_test_feat = np.array([extract_statistical_features(x) for x in tqdm(X_test, desc=\"Test features\")])\n",
    "    \n",
    "    # Normalize\n",
    "    scaler = StandardScaler()\n",
    "    X_train_feat = scaler.fit_transform(X_train_feat)\n",
    "    X_test_feat = scaler.transform(X_test_feat)\n",
    "    \n",
    "    # Replace NaN with 0\n",
    "    X_train_feat = np.nan_to_num(X_train_feat)\n",
    "    X_test_feat = np.nan_to_num(X_test_feat)\n",
    "    \n",
    "    mlp_results = []\n",
    "    \n",
    "    # MLP configurations\n",
    "    mlp_configs = [\n",
    "        {\"hidden\": (128, 64), \"dropout\": 0.2, \"lr\": 1e-3, \"epochs\": 30},\n",
    "        {\"hidden\": (256, 128), \"dropout\": 0.2, \"lr\": 1e-3, \"epochs\": 30},\n",
    "        {\"hidden\": (256, 128, 64), \"dropout\": 0.3, \"lr\": 5e-4, \"epochs\": 40},\n",
    "    ]\n",
    "    \n",
    "    print(f\"\\nğŸ”„ Training {len(mlp_configs)} MLP configurations...\")\n",
    "    \n",
    "    for i, cfg in enumerate(mlp_configs):\n",
    "        print(f\"\\n  Config {i+1}/{len(mlp_configs)}: hidden={cfg['hidden']}\")\n",
    "        \n",
    "        # Convert to tensors\n",
    "        Xtr_t = torch.tensor(X_train_feat, dtype=torch.float32)\n",
    "        ytr_t = torch.tensor(y_train, dtype=torch.long)\n",
    "        Xte_t = torch.tensor(X_test_feat, dtype=torch.float32)\n",
    "        \n",
    "        train_ds = torch.utils.data.TensorDataset(Xtr_t, ytr_t)\n",
    "        train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
    "        \n",
    "        # Build MLP\n",
    "        class SimpleMLP(nn.Module):\n",
    "            def __init__(self, input_dim, hidden_dims, num_classes, dropout=0.2):\n",
    "                super().__init__()\n",
    "                layers = []\n",
    "                d = input_dim\n",
    "                for h in hidden_dims:\n",
    "                    layers += [nn.Linear(d, h), nn.ReLU(), nn.Dropout(dropout)]\n",
    "                    d = h\n",
    "                layers += [nn.Linear(d, num_classes)]\n",
    "                self.net = nn.Sequential(*layers)\n",
    "            def forward(self, x):\n",
    "                return self.net(x)\n",
    "        \n",
    "        model = SimpleMLP(X_train_feat.shape[1], cfg[\"hidden\"], len(CLASS_NAMES), cfg[\"dropout\"]).to(DEVICE)\n",
    "        opt = torch.optim.Adam(model.parameters(), lr=cfg[\"lr\"])\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "        \n",
    "        # Training with progress bar\n",
    "        for epoch in tqdm(range(cfg[\"epochs\"]), desc=f\"  Training\", leave=False):\n",
    "            model.train()\n",
    "            for xb, yb in train_loader:\n",
    "                xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "                opt.zero_grad()\n",
    "                loss = loss_fn(model(xb), yb)\n",
    "                loss.backward()\n",
    "                opt.step()\n",
    "        \n",
    "        # Evaluate\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            y_pred = model(Xte_t.to(DEVICE)).argmax(dim=1).cpu().numpy()\n",
    "        \n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred, average=\"macro\")\n",
    "        \n",
    "        result = {\n",
    "            \"method\": \"StatFeatures+MLP\",\n",
    "            \"hidden\": str(cfg[\"hidden\"]),\n",
    "            \"dropout\": cfg[\"dropout\"],\n",
    "            \"lr\": cfg[\"lr\"],\n",
    "            \"accuracy\": acc,\n",
    "            \"macro_f1\": f1\n",
    "        }\n",
    "        mlp_results.append(result)\n",
    "        RESULTS.append(result)\n",
    "        print(f\"    â†’ acc={acc:.4f}, macro_f1={f1:.4f}\")\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"\\nâœ… Method 2 complete in {format_time(elapsed)}\")\n",
    "    \n",
    "    # Save best confusion matrix\n",
    "    best_result = max(mlp_results, key=lambda x: x[\"macro_f1\"])\n",
    "    print(f\"Best config: {best_result['hidden']}\")\n",
    "    evaluate_and_report(y_test, y_pred, title=\"StatFeatures+MLP (best)\", \n",
    "                        save_cm_path=FIG_DIR/\"cm_stat_mlp.png\")\n",
    "    \n",
    "    save_checkpoint(\"method2_mlp\", mlp_results)\n",
    "else:\n",
    "    print(\"âœ… Loaded from checkpoint!\")\n",
    "    RESULTS.extend(mlp_results)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# METHOD 3: LSTM Classifier\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"âš¡ METHOD 3: LSTM Classifier\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "lstm_results = load_checkpoint(\"method3_lstm\")\n",
    "\n",
    "if lstm_results is None:\n",
    "    start_time = time.time()\n",
    "    \n",
    "    class PoseDataset(Dataset):\n",
    "        def __init__(self, samples):\n",
    "            self.samples = samples\n",
    "        def __len__(self):\n",
    "            return len(self.samples)\n",
    "        def __getitem__(self, idx):\n",
    "            s = self.samples[idx]\n",
    "            x = torch.tensor(s[\"X\"], dtype=torch.float32)  # Use \"X\" key\n",
    "            y = torch.tensor(s[\"y\"], dtype=torch.long)     # Use \"y\" key\n",
    "            return x, y\n",
    "    \n",
    "    def collate_fn(batch):\n",
    "        xs, ys = zip(*batch)\n",
    "        lengths = torch.tensor([len(x) for x in xs])\n",
    "        xs_padded = pad_sequence(xs, batch_first=True)\n",
    "        ys = torch.stack(ys)\n",
    "        return xs_padded, ys, lengths\n",
    "    \n",
    "    class LSTMClassifier(nn.Module):\n",
    "        def __init__(self, input_dim, hidden_dim, num_layers, num_classes, dropout=0.2, bidirectional=False):\n",
    "            super().__init__()\n",
    "            self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, \n",
    "                               batch_first=True, dropout=dropout if num_layers > 1 else 0,\n",
    "                               bidirectional=bidirectional)\n",
    "            self.dropout = nn.Dropout(dropout)\n",
    "            mult = 2 if bidirectional else 1\n",
    "            self.fc = nn.Linear(hidden_dim * mult, num_classes)\n",
    "        \n",
    "        def forward(self, x, lengths):\n",
    "            # Pack sequence\n",
    "            packed = pack_padded_sequence(x, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "            _, (hn, _) = self.lstm(packed)\n",
    "            # Get last hidden state\n",
    "            if self.lstm.bidirectional:\n",
    "                hn = torch.cat([hn[-2], hn[-1]], dim=1)\n",
    "            else:\n",
    "                hn = hn[-1]\n",
    "            out = self.dropout(hn)\n",
    "            return self.fc(out)\n",
    "    \n",
    "    train_ds = PoseDataset(train_samples)\n",
    "    test_ds = PoseDataset(test_samples)\n",
    "    train_loader = DataLoader(train_ds, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "    test_loader = DataLoader(test_ds, batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
    "    \n",
    "    input_dim = train_samples[0][\"X\"].shape[1]  # Use \"X\" key\n",
    "    \n",
    "    lstm_results = []\n",
    "    lstm_configs = [\n",
    "        {\"hidden\": 64, \"layers\": 1, \"dropout\": 0.2, \"bidir\": False, \"epochs\": 20},\n",
    "        {\"hidden\": 128, \"layers\": 1, \"dropout\": 0.2, \"bidir\": False, \"epochs\": 20},\n",
    "        {\"hidden\": 64, \"layers\": 2, \"dropout\": 0.3, \"bidir\": False, \"epochs\": 25},\n",
    "        {\"hidden\": 64, \"layers\": 1, \"dropout\": 0.2, \"bidir\": True, \"epochs\": 20},\n",
    "    ]\n",
    "    \n",
    "    print(f\"ğŸ”„ Training {len(lstm_configs)} LSTM configurations...\")\n",
    "    \n",
    "    for i, cfg in enumerate(lstm_configs):\n",
    "        print(f\"\\n  Config {i+1}/{len(lstm_configs)}: hidden={cfg['hidden']}, layers={cfg['layers']}, bidir={cfg['bidir']}\")\n",
    "        \n",
    "        model = LSTMClassifier(\n",
    "            input_dim, cfg[\"hidden\"], cfg[\"layers\"], \n",
    "            len(CLASS_NAMES), cfg[\"dropout\"], cfg[\"bidir\"]\n",
    "        ).to(DEVICE)\n",
    "        \n",
    "        opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "        \n",
    "        for epoch in tqdm(range(cfg[\"epochs\"]), desc=f\"  Training\", leave=False):\n",
    "            model.train()\n",
    "            for xb, yb, lengths in train_loader:\n",
    "                xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "                opt.zero_grad()\n",
    "                logits = model(xb, lengths)\n",
    "                loss = loss_fn(logits, yb)\n",
    "                loss.backward()\n",
    "                opt.step()\n",
    "        \n",
    "        # Evaluate\n",
    "        model.eval()\n",
    "        all_pred, all_true = [], []\n",
    "        with torch.no_grad():\n",
    "            for xb, yb, lengths in test_loader:\n",
    "                xb = xb.to(DEVICE)\n",
    "                pred = model(xb, lengths).argmax(dim=1).cpu().numpy()\n",
    "                all_pred.extend(pred)\n",
    "                all_true.extend(yb.numpy())\n",
    "        \n",
    "        acc = accuracy_score(all_true, all_pred)\n",
    "        f1 = f1_score(all_true, all_pred, average=\"macro\")\n",
    "        \n",
    "        result = {\n",
    "            \"method\": \"LSTM\",\n",
    "            \"hidden\": cfg[\"hidden\"],\n",
    "            \"layers\": cfg[\"layers\"],\n",
    "            \"bidirectional\": cfg[\"bidir\"],\n",
    "            \"dropout\": cfg[\"dropout\"],\n",
    "            \"accuracy\": acc,\n",
    "            \"macro_f1\": f1\n",
    "        }\n",
    "        lstm_results.append(result)\n",
    "        RESULTS.append(result)\n",
    "        print(f\"    â†’ acc={acc:.4f}, macro_f1={f1:.4f}\")\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"\\nâœ… Method 3 complete in {format_time(elapsed)}\")\n",
    "    \n",
    "    # Save best confusion matrix\n",
    "    best_result = max(lstm_results, key=lambda x: x[\"macro_f1\"])\n",
    "    evaluate_and_report(all_true, all_pred, title=\"LSTM (best)\", \n",
    "                        save_cm_path=FIG_DIR/\"cm_lstm.png\")\n",
    "    \n",
    "    save_checkpoint(\"method3_lstm\", lstm_results)\n",
    "else:\n",
    "    print(\"âœ… Loaded from checkpoint!\")\n",
    "    RESULTS.extend(lstm_results)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# METHOD 4: 1D Temporal CNN (Extra Method)\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"âš¡ METHOD 4: Temporal CNN (Extra Method)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "cnn_results = load_checkpoint(\"method4_cnn\")\n",
    "\n",
    "if cnn_results is None:\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Fixed-length sequences for CNN\n",
    "    TARGET_LEN = 100\n",
    "    \n",
    "    def prepare_for_cnn(samples, target_len=TARGET_LEN):\n",
    "        X, y = [], []\n",
    "        for s in samples:\n",
    "            seq = s[\"X\"]  # Use \"X\" key\n",
    "            # Resample to fixed length\n",
    "            if len(seq) >= target_len:\n",
    "                indices = np.linspace(0, len(seq)-1, target_len, dtype=int)\n",
    "                seq = seq[indices]\n",
    "            else:\n",
    "                # Pad\n",
    "                padded = np.zeros((target_len, seq.shape[1]))\n",
    "                padded[:len(seq)] = seq\n",
    "                seq = padded\n",
    "            X.append(seq)\n",
    "            y.append(s[\"y\"])  # Use \"y\" key\n",
    "        return np.array(X), np.array(y)\n",
    "    \n",
    "    print(\"ğŸ“Š Preparing fixed-length sequences for CNN...\")\n",
    "    X_train_cnn, y_train_cnn = prepare_for_cnn(train_samples)\n",
    "    X_test_cnn, y_test_cnn = prepare_for_cnn(test_samples)\n",
    "    \n",
    "    # Transpose to [N, C, T] for Conv1d (channels = features)\n",
    "    X_train_cnn = X_train_cnn.transpose(0, 2, 1)\n",
    "    X_test_cnn = X_test_cnn.transpose(0, 2, 1)\n",
    "    \n",
    "    class TemporalCNN(nn.Module):\n",
    "        def __init__(self, input_channels, num_classes, hidden_channels=64):\n",
    "            super().__init__()\n",
    "            self.conv1 = nn.Conv1d(input_channels, hidden_channels, kernel_size=5, padding=2)\n",
    "            self.bn1 = nn.BatchNorm1d(hidden_channels)\n",
    "            self.conv2 = nn.Conv1d(hidden_channels, hidden_channels*2, kernel_size=5, padding=2)\n",
    "            self.bn2 = nn.BatchNorm1d(hidden_channels*2)\n",
    "            self.conv3 = nn.Conv1d(hidden_channels*2, hidden_channels*2, kernel_size=3, padding=1)\n",
    "            self.bn3 = nn.BatchNorm1d(hidden_channels*2)\n",
    "            self.pool = nn.AdaptiveAvgPool1d(1)\n",
    "            self.dropout = nn.Dropout(0.3)\n",
    "            self.fc = nn.Linear(hidden_channels*2, num_classes)\n",
    "        \n",
    "        def forward(self, x):\n",
    "            x = torch.relu(self.bn1(self.conv1(x)))\n",
    "            x = torch.max_pool1d(x, 2)\n",
    "            x = torch.relu(self.bn2(self.conv2(x)))\n",
    "            x = torch.max_pool1d(x, 2)\n",
    "            x = torch.relu(self.bn3(self.conv3(x)))\n",
    "            x = self.pool(x).squeeze(-1)\n",
    "            x = self.dropout(x)\n",
    "            return self.fc(x)\n",
    "    \n",
    "    # Convert to tensors\n",
    "    Xtr_t = torch.tensor(X_train_cnn, dtype=torch.float32)\n",
    "    ytr_t = torch.tensor(y_train_cnn, dtype=torch.long)\n",
    "    Xte_t = torch.tensor(X_test_cnn, dtype=torch.float32)\n",
    "    \n",
    "    train_ds = torch.utils.data.TensorDataset(Xtr_t, ytr_t)\n",
    "    train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
    "    \n",
    "    cnn_results = []\n",
    "    cnn_configs = [\n",
    "        {\"hidden\": 32, \"epochs\": 25},\n",
    "        {\"hidden\": 64, \"epochs\": 25},\n",
    "        {\"hidden\": 128, \"epochs\": 30},\n",
    "    ]\n",
    "    \n",
    "    print(f\"ğŸ”„ Training {len(cnn_configs)} CNN configurations...\")\n",
    "    \n",
    "    for i, cfg in enumerate(cnn_configs):\n",
    "        print(f\"\\n  Config {i+1}/{len(cnn_configs)}: hidden_channels={cfg['hidden']}\")\n",
    "        \n",
    "        model = TemporalCNN(X_train_cnn.shape[1], len(CLASS_NAMES), cfg[\"hidden\"]).to(DEVICE)\n",
    "        opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "        \n",
    "        for epoch in tqdm(range(cfg[\"epochs\"]), desc=f\"  Training\", leave=False):\n",
    "            model.train()\n",
    "            for xb, yb in train_loader:\n",
    "                xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "                opt.zero_grad()\n",
    "                loss = loss_fn(model(xb), yb)\n",
    "                loss.backward()\n",
    "                opt.step()\n",
    "        \n",
    "        # Evaluate\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            y_pred = model(Xte_t.to(DEVICE)).argmax(dim=1).cpu().numpy()\n",
    "        \n",
    "        acc = accuracy_score(y_test_cnn, y_pred)\n",
    "        f1 = f1_score(y_test_cnn, y_pred, average=\"macro\")\n",
    "        \n",
    "        result = {\n",
    "            \"method\": \"TemporalCNN\",\n",
    "            \"hidden_channels\": cfg[\"hidden\"],\n",
    "            \"accuracy\": acc,\n",
    "            \"macro_f1\": f1\n",
    "        }\n",
    "        cnn_results.append(result)\n",
    "        RESULTS.append(result)\n",
    "        print(f\"    â†’ acc={acc:.4f}, macro_f1={f1:.4f}\")\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"\\nâœ… Method 4 complete in {format_time(elapsed)}\")\n",
    "    \n",
    "    evaluate_and_report(y_test_cnn, y_pred, title=\"TemporalCNN (best)\", \n",
    "                        save_cm_path=FIG_DIR/\"cm_temporal_cnn.png\")\n",
    "    \n",
    "    save_checkpoint(\"method4_cnn\", cnn_results)\n",
    "else:\n",
    "    print(\"âœ… Loaded from checkpoint!\")\n",
    "    RESULTS.extend(cnn_results)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# FINAL RESULTS SUMMARY\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ“Š FINAL RESULTS SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Save all results\n",
    "df_all = pd.DataFrame(RESULTS)\n",
    "df_all.to_csv(RES_DIR / \"all_results.csv\", index=False)\n",
    "print(f\"\\nğŸ’¾ All results saved to: {RES_DIR / 'all_results.csv'}\")\n",
    "\n",
    "# Display summary by method\n",
    "print(\"\\nğŸ“ˆ Best result per method:\")\n",
    "for method in df_all[\"method\"].unique():\n",
    "    df_method = df_all[df_all[\"method\"] == method]\n",
    "    best = df_method.loc[df_method[\"macro_f1\"].idxmax()]\n",
    "    print(f\"  {method:20s}: acc={best['accuracy']:.4f}, macro_f1={best['macro_f1']:.4f}\")\n",
    "\n",
    "# Comparison plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "method_best = df_all.groupby(\"method\")[\"macro_f1\"].max().sort_values(ascending=True)\n",
    "colors = plt.cm.viridis(np.linspace(0.2, 0.8, len(method_best)))\n",
    "bars = plt.barh(method_best.index, method_best.values, color=colors)\n",
    "plt.xlabel(\"Macro F1 Score\")\n",
    "plt.title(\"Comparison of All Methods (Best Configuration)\")\n",
    "for bar, val in zip(bars, method_best.values):\n",
    "    plt.text(val + 0.01, bar.get_y() + bar.get_height()/2, f\"{val:.3f}\", va='center')\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIG_DIR / \"method_comparison.png\", dpi=200)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ… ALL METHODS COMPLETE!\")\n",
    "print(f\"ğŸ“ Results saved in: {OUT_DIR}\")\n",
    "print(f\"ğŸ“ Checkpoints saved in: {CHECKPOINT_DIR}\")\n",
    "print(\"\\nğŸ’¡ To resume on another computer:\")\n",
    "print(f\"   1. Copy the entire '{OUT_DIR}' folder\")\n",
    "print(f\"   2. Run this cell - it will load from checkpoints automatically!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# METHOD 1 â€” GAK + SVM (required)\n",
    "\n",
    "**Owners:** Person B (primary), Person A (review)\n",
    "\n",
    "### Checklist\n",
    "- [X] Prepare sequences for tslearn (`[N, T, D]`, [T, D] = one videoâ€™s sequence.\n",
    "[N, T, D] = all videos stacked together.)\n",
    "- [X] Compute GAK Gram matrix for train (`K_train`)\n",
    "- [X] Train SVM on precomputed kernel\n",
    "- [X] Compute `K_test` and predict\n",
    "- [X] Log metrics + save confusion matrix\n",
    "- [X] Ablation study:\n",
    "  - [X] `sigma` (kernel bandwidth) sweep\n",
    "  - [X] SVM `C` sweep\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 26\u001b[39m\n\u001b[32m     23\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m y_test, y_pred\n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m# Quick single run (edit params)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m y_true, y_pred = \u001b[43mgak_svm_train_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msigma\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mC\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1.0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m metrics = evaluate_and_report(y_true, y_pred, title=\u001b[33m\"\u001b[39m\u001b[33mGAK+SVM\u001b[39m\u001b[33m\"\u001b[39m, save_cm_path=FIG_DIR/\u001b[33m\"\u001b[39m\u001b[33mcm_gak_svm.png\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     28\u001b[39m RESULTS.append({\u001b[33m\"\u001b[39m\u001b[33mmethod\u001b[39m\u001b[33m\"\u001b[39m:\u001b[33m\"\u001b[39m\u001b[33mGAK+SVM\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33msigma\u001b[39m\u001b[33m\"\u001b[39m:\u001b[32m1.0\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mC\u001b[39m\u001b[33m\"\u001b[39m:\u001b[32m1.0\u001b[39m, **metrics})\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 17\u001b[39m, in \u001b[36mgak_svm_train_predict\u001b[39m\u001b[34m(train_samples, test_samples, sigma, C)\u001b[39m\n\u001b[32m     14\u001b[39m Xte_s = scaler_te.fit_transform(Xte)\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# GAK Gram matrices\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m K_train = \u001b[43mcdist_gak\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXtr_s\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mXtr_s\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msigma\u001b[49m\u001b[43m=\u001b[49m\u001b[43msigma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m K_test  = cdist_gak(Xte_s, Xtr_s, sigma=sigma)\n\u001b[32m     20\u001b[39m clf = SVC(kernel=\u001b[33m\"\u001b[39m\u001b[33mprecomputed\u001b[39m\u001b[33m\"\u001b[39m, C=C)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Filiz Beyza\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tslearn\\metrics\\softdtw_variants.py:324\u001b[39m, in \u001b[36mcdist_gak\u001b[39m\u001b[34m(dataset1, dataset2, sigma, n_jobs, verbose, be)\u001b[39m\n\u001b[32m    259\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Compute cross-similarity matrix using Global Alignment kernel (GAK).\u001b[39;00m\n\u001b[32m    260\u001b[39m \n\u001b[32m    261\u001b[39m \u001b[33;03mGAK was originally presented in [1]_.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    320\u001b[39m \u001b[33;03m   ICML 2011.\u001b[39;00m\n\u001b[32m    321\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m  \u001b[38;5;66;03m# noqa: E501\u001b[39;00m\n\u001b[32m    322\u001b[39m be = instantiate_backend(be, dataset1, dataset2)\n\u001b[32m--> \u001b[39m\u001b[32m324\u001b[39m unnormalized_matrix = \u001b[43m_cdist_generic\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    325\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdist_fun\u001b[49m\u001b[43m=\u001b[49m\u001b[43munnormalized_gak\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    326\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdataset1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdataset1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    327\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdataset2\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdataset2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    328\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    329\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    330\u001b[39m \u001b[43m    \u001b[49m\u001b[43msigma\u001b[49m\u001b[43m=\u001b[49m\u001b[43msigma\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    331\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompute_diagonal\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    332\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbe\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbe\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    333\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    334\u001b[39m dataset1 = to_time_series_dataset(dataset1, be=be)\n\u001b[32m    335\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m dataset2 \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Filiz Beyza\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tslearn\\metrics\\utils.py:101\u001b[39m, in \u001b[36m_cdist_generic\u001b[39m\u001b[34m(dist_fun, dataset1, dataset2, n_jobs, verbose, compute_diagonal, dtype, be, *args, **kwargs)\u001b[39m\n\u001b[32m     99\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    100\u001b[39m     dataset2 = to_time_series_dataset(dataset2, dtype=dtype, be=be)\n\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m     matrix = \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefer\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mthreads\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    102\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdist_fun\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset1\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset2\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    103\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataset1\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    104\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataset2\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    105\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    106\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m be.reshape(be.array(matrix), (\u001b[38;5;28mlen\u001b[39m(dataset1), -\u001b[32m1\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Filiz Beyza\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\joblib\\parallel.py:1986\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   1984\u001b[39m     output = \u001b[38;5;28mself\u001b[39m._get_sequential_output(iterable)\n\u001b[32m   1985\u001b[39m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[32m-> \u001b[39m\u001b[32m1986\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1988\u001b[39m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[32m   1989\u001b[39m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[32m   1990\u001b[39m \u001b[38;5;66;03m# reused, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[32m   1991\u001b[39m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[32m   1992\u001b[39m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[32m   1993\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._lock:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Filiz Beyza\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\joblib\\parallel.py:1914\u001b[39m, in \u001b[36mParallel._get_sequential_output\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   1912\u001b[39m \u001b[38;5;28mself\u001b[39m.n_dispatched_batches += \u001b[32m1\u001b[39m\n\u001b[32m   1913\u001b[39m \u001b[38;5;28mself\u001b[39m.n_dispatched_tasks += \u001b[32m1\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1914\u001b[39m res = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1915\u001b[39m \u001b[38;5;28mself\u001b[39m.n_completed_tasks += \u001b[32m1\u001b[39m\n\u001b[32m   1916\u001b[39m \u001b[38;5;28mself\u001b[39m.print_progress()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Filiz Beyza\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tslearn\\metrics\\softdtw_variants.py:192\u001b[39m, in \u001b[36munnormalized_gak\u001b[39m\u001b[34m(s1, s2, sigma, be)\u001b[39m\n\u001b[32m    189\u001b[39m s1 = to_time_series(s1, remove_nans=\u001b[38;5;28;01mTrue\u001b[39;00m, be=be)\n\u001b[32m    190\u001b[39m s2 = to_time_series(s2, remove_nans=\u001b[38;5;28;01mTrue\u001b[39;00m, be=be)\n\u001b[32m--> \u001b[39m\u001b[32m192\u001b[39m gram = \u001b[43m_gak_gram\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ms2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msigma\u001b[49m\u001b[43m=\u001b[49m\u001b[43msigma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbe\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbe\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    194\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m be.is_numpy:\n\u001b[32m    195\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _njit_gak(gram)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Filiz Beyza\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tslearn\\metrics\\softdtw_variants.py:132\u001b[39m, in \u001b[36m_gak_gram\u001b[39m\u001b[34m(s1, s2, sigma, be)\u001b[39m\n\u001b[32m    130\u001b[39m gram = -be.cdist(s1, s2, \u001b[33m\"\u001b[39m\u001b[33msqeuclidean\u001b[39m\u001b[33m\"\u001b[39m) / (\u001b[32m2\u001b[39m * sigma**\u001b[32m2\u001b[39m)\n\u001b[32m    131\u001b[39m gram = be.array(gram)\n\u001b[32m--> \u001b[39m\u001b[32m132\u001b[39m gram -= \u001b[43mbe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlog\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[43mbe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgram\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    133\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m be.exp(gram)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "def gak_svm_train_predict(train_samples, test_samples, sigma=1.0, C=1.0):\n",
    "    # Prepare\n",
    "    X_train, y_train = get_xy(train_samples)\n",
    "    X_test,  y_test  = get_xy(test_samples)\n",
    "\n",
    "    Xtr = to_time_series_dataset(X_train)  # [N, T, D] with padding by tslearn\n",
    "    Xte = to_time_series_dataset(X_test)\n",
    "\n",
    "    # Scale each dataset independently (per-series normalization)\n",
    "    # Use separate scalers since train/test can have different lengths\n",
    "    scaler_tr = TimeSeriesScalerMeanVariance()\n",
    "    scaler_te = TimeSeriesScalerMeanVariance()\n",
    "    Xtr_s = scaler_tr.fit_transform(Xtr)\n",
    "    Xte_s = scaler_te.fit_transform(Xte)\n",
    "\n",
    "    # GAK Gram matrices\n",
    "    K_train = cdist_gak(Xtr_s, Xtr_s, sigma=sigma)\n",
    "    K_test  = cdist_gak(Xte_s, Xtr_s, sigma=sigma)\n",
    "\n",
    "    clf = SVC(kernel=\"precomputed\", C=C)\n",
    "    clf.fit(K_train, y_train)\n",
    "    y_pred = clf.predict(K_test)\n",
    "    return y_test, y_pred\n",
    "\n",
    "# Quick single run (edit params)\n",
    "y_true, y_pred = gak_svm_train_predict(train_samples, test_samples, sigma=1.0, C=1.0)\n",
    "metrics = evaluate_and_report(y_true, y_pred, title=\"GAK+SVM\", save_cm_path=FIG_DIR/\"cm_gak_svm.png\")\n",
    "RESULTS.append({\"method\":\"GAK+SVM\", \"sigma\":1.0, \"C\":1.0, **metrics})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dadc79f",
   "metadata": {},
   "source": [
    "## METHOD 1 â€” Ablation grid (GAK+SVM)\n",
    "\n",
    "**Owners:** Person B\n",
    "\n",
    "### Checklist\n",
    "- [x] Define a reasonable grid (6-12 runs total)\n",
    "- [x] Save results to CSV\n",
    "- [x] Plot performance vs sigma and vs C\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef401221",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_gak_svm_ablation(train_samples, test_samples, sigmas, Cs):\n",
    "    existing = {\n",
    "        (r.get(\"method\"), r.get(\"sigma\"), r.get(\"C\"))\n",
    "        for r in RESULTS\n",
    "        if r.get(\"method\") == \"GAK+SVM\"\n",
    "    }\n",
    "    for sigma in sigmas:\n",
    "        for C in Cs:\n",
    "            if (\"GAK+SVM\", sigma, C) in existing:\n",
    "                continue\n",
    "            y_true, y_pred = gak_svm_train_predict(train_samples, test_samples, sigma=sigma, C=C)\n",
    "            title = f\"GAK+SVM sigma={sigma} C={C}\"\n",
    "            cm_path = FIG_DIR / f\"cm_gak_svm_sigma{sigma}_C{C}.png\"\n",
    "            metrics = evaluate_and_report(y_true, y_pred, title=title, save_cm_path=cm_path)\n",
    "            RESULTS.append({\"method\":\"GAK+SVM\", \"sigma\":sigma, \"C\":C, **metrics})\n",
    "\n",
    "# Ablation grid (12 runs)\n",
    "# Tune these if runtime is too slow.\n",
    "sigmas = [0.25, 0.5, 1.0, 2.0]\n",
    "Cs = [0.1, 1.0, 10.0]\n",
    "run_gak_svm_ablation(train_samples, test_samples, sigmas, Cs)\n",
    "\n",
    "# Save results table (GAK only)\n",
    "df_gak = pd.DataFrame([r for r in RESULTS if r.get(\"method\") == \"GAK+SVM\"])\n",
    "df_gak.to_csv(RES_DIR / \"results_gak_svm_ablation.csv\", index=False)\n",
    "\n",
    "# Plot performance vs sigma\n",
    "sigma_perf = df_gak.groupby(\"sigma\")[\"macro_f1\"].mean().reset_index().sort_values(\"sigma\")\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(sigma_perf[\"sigma\"], sigma_perf[\"macro_f1\"], marker=\"o\")\n",
    "plt.xlabel(\"sigma\")\n",
    "plt.ylabel(\"macro_f1\")\n",
    "plt.title(\"GAK+SVM ablation: macro_f1 vs sigma\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIG_DIR / \"gak_svm_ablation_sigma.png\", dpi=200)\n",
    "plt.show()\n",
    "\n",
    "# Plot performance vs C\n",
    "c_perf = df_gak.groupby(\"C\")[\"macro_f1\"].mean().reset_index().sort_values(\"C\")\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(c_perf[\"C\"], c_perf[\"macro_f1\"], marker=\"o\")\n",
    "plt.xlabel(\"C\")\n",
    "plt.ylabel(\"macro_f1\")\n",
    "plt.title(\"GAK+SVM ablation: macro_f1 vs C\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIG_DIR / \"gak_svm_ablation_C.png\", dpi=200)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# METHOD 2 â€” Shapelets + MLP (required)\n",
    "\n",
    "**Owners:** Person A (primary for shapelets), Person B (primary for PyTorch MLP)\n",
    "\n",
    "### Checklist\n",
    "- [ ] Fit shapelet transform/model on training set\n",
    "- [ ] Transform train/test to fixed-length feature vectors\n",
    "- [ ] Train MLP classifier (PyTorch)\n",
    "- [ ] Evaluate + confusion matrix\n",
    "- [ ] Ablations:\n",
    "  - [ ] shapelet sizes / counts\n",
    "  - [ ] MLP hidden size / depth / dropout\n",
    "  - [ ] learning rate / epochs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shapelets imports (may fail if tslearn version differs)\n",
    "try:\n",
    "    from tslearn.shapelets import ShapeletModel\n",
    "except Exception as e:\n",
    "    print(\"ShapeletModel import issue:\", e)\n",
    "\n",
    "def build_shapelet_datasets(train_samples, test_samples):\n",
    "    # For shapelets, use [N,T,D] with possible scaling\n",
    "    X_train, y_train = get_xy(train_samples)\n",
    "    X_test,  y_test  = get_xy(test_samples)\n",
    "    Xtr = to_time_series_dataset(X_train)\n",
    "    Xte = to_time_series_dataset(X_test)\n",
    "    # Scale each dataset independently (per-series normalization)\n",
    "    scaler_tr = TimeSeriesScalerMeanVariance()\n",
    "    scaler_te = TimeSeriesScalerMeanVariance()\n",
    "    Xtr_s = scaler_tr.fit_transform(Xtr)\n",
    "    Xte_s = scaler_te.fit_transform(Xte)\n",
    "    return Xtr_s, y_train, Xte_s, y_test\n",
    "\n",
    "def fit_shapelets(Xtr, ytr, n_shapelets_per_size, max_iter=50):\n",
    "    \"\"\"Fit shapelet model. You must tune sizes/counts.\"\"\"\n",
    "    # TODO (Person A): choose params based on assignment + ablations\n",
    "    shp = ShapeletModel(\n",
    "        n_shapelets_per_size=n_shapelets_per_size,\n",
    "        optimizer=\"adam\",\n",
    "        weight_regularizer=0.01,\n",
    "        max_iter=max_iter,\n",
    "        verbose=1,\n",
    "        random_state=SEED\n",
    "    )\n",
    "    shp.fit(Xtr, ytr)\n",
    "    return shp\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, num_classes, dropout=0.2):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        d = input_dim\n",
    "        for h in hidden_dims:\n",
    "            layers += [nn.Linear(d, h), nn.ReLU(), nn.Dropout(dropout)]\n",
    "            d = h\n",
    "        layers += [nn.Linear(d, num_classes)]\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "def train_mlp_on_features(Xtr_feat, ytr, Xte_feat, yte, hidden_dims=(256,128), lr=1e-3, epochs=30, batch_size=64, dropout=0.2):\n",
    "    Xtr_t = torch.tensor(Xtr_feat, dtype=torch.float32)\n",
    "    ytr_t = torch.tensor(ytr, dtype=torch.long)\n",
    "    Xte_t = torch.tensor(Xte_feat, dtype=torch.float32)\n",
    "    yte_t = torch.tensor(yte, dtype=torch.long)\n",
    "\n",
    "    train_ds = torch.utils.data.TensorDataset(Xtr_t, ytr_t)\n",
    "    test_ds  = torch.utils.data.TensorDataset(Xte_t, yte_t)\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "    test_loader  = DataLoader(test_ds, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    model = MLP(Xtr_feat.shape[1], list(hidden_dims), len(CLASS_NAMES), dropout=dropout).to(DEVICE)\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    history = {\"train_loss\":[], \"train_acc\":[], \"test_acc\":[]}\n",
    "\n",
    "    for ep in range(1, epochs+1):\n",
    "        model.train()\n",
    "        total_loss=0.0\n",
    "        correct=0\n",
    "        n=0\n",
    "        for xb, yb in train_loader:\n",
    "            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "            opt.zero_grad()\n",
    "            logits = model(xb)\n",
    "            loss = loss_fn(logits, yb)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            total_loss += loss.item()*len(xb)\n",
    "            pred = logits.argmax(dim=1)\n",
    "            correct += (pred==yb).sum().item()\n",
    "            n += len(xb)\n",
    "\n",
    "        train_loss = total_loss/n\n",
    "        train_acc = correct/n\n",
    "\n",
    "        model.eval()\n",
    "        all_pred=[]\n",
    "        all_true=[]\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in test_loader:\n",
    "                xb = xb.to(DEVICE)\n",
    "                logits = model(xb)\n",
    "                pred = logits.argmax(dim=1).cpu().numpy()\n",
    "                all_pred.append(pred)\n",
    "                all_true.append(yb.numpy())\n",
    "        all_pred = np.concatenate(all_pred)\n",
    "        all_true = np.concatenate(all_true)\n",
    "        test_acc = accuracy_score(all_true, all_pred)\n",
    "\n",
    "        history[\"train_loss\"].append(train_loss)\n",
    "        history[\"train_acc\"].append(train_acc)\n",
    "        history[\"test_acc\"].append(test_acc)\n",
    "\n",
    "        if ep % max(1, epochs//10) == 0 or ep == 1:\n",
    "            print(f\"epoch {ep:03d} | loss {train_loss:.4f} | train_acc {train_acc:.4f} | test_acc {test_acc:.4f}\")\n",
    "\n",
    "    return model, history, all_true, all_pred\n",
    "\n",
    "# Pipeline template (uncomment after implementing shapelet transform properly)\n",
    "# Xtr_s, ytr, Xte_s, yte = build_shapelet_datasets(train_samples, test_samples)\n",
    "# shapelet_model = fit_shapelets(Xtr_s, ytr, n_shapelets_per_size={10:5, 20:5}, max_iter=50)\n",
    "# Xtr_feat = shapelet_model.transform(Xtr_s)\n",
    "# Xte_feat = shapelet_model.transform(Xte_s)\n",
    "# mlp_model, hist, y_true, y_pred = train_mlp_on_features(Xtr_feat, ytr, Xte_feat, yte)\n",
    "# metrics = evaluate_and_report(y_true, y_pred, title=\"Shapelets+MLP\", save_cm_path=FIG_DIR/\"cm_shapelets_mlp.png\")\n",
    "# RESULTS.append({\"method\":\"Shapelets+MLP\", **metrics})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## METHOD 2 â€” Ablation grid (Shapelets+MLP)\n",
    "\n",
    "**Owners:** Person A (shapelet config sweep), Person B (MLP sweep)\n",
    "\n",
    "### Checklist\n",
    "- [X] Define shapelet configs:\n",
    "  - [X] sizes (e.g., 10/20/30)\n",
    "  - [X] counts per size (small/med/large)\n",
    "  - [X] max_iter\n",
    "- [X] Define MLP configs:\n",
    "  - [X] hidden_dims, dropout\n",
    "  - [X] lr, epochs\n",
    "- [X] Run a controlled grid and log results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shapelets + MLP Ablation Runner\n",
    "# Person A: shapelet config sweep\n",
    "# Person B: MLP config sweep\n",
    "\n",
    "def run_shapelets_mlp_ablation(train_samples, test_samples, shapelet_configs, mlp_configs):\n",
    "    \"\"\"\n",
    "    Run Shapelets+MLP experiments with different configurations.\n",
    "    \"\"\"\n",
    "    # Build base dataset\n",
    "    Xtr_s, ytr, Xte_s, yte = build_shapelet_datasets(train_samples, test_samples)\n",
    "    \n",
    "    results_local = []\n",
    "    \n",
    "    for shp_cfg in shapelet_configs:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Training Shapelet model: sizes={shp_cfg['n_shapelets_per_size']}, max_iter={shp_cfg['max_iter']}\")\n",
    "        print('='*60)\n",
    "        \n",
    "        try:\n",
    "            # Fit shapelets\n",
    "            shapelet_model = fit_shapelets(\n",
    "                Xtr_s, ytr,\n",
    "                n_shapelets_per_size=shp_cfg['n_shapelets_per_size'],\n",
    "                max_iter=shp_cfg['max_iter']\n",
    "            )\n",
    "            \n",
    "            # Transform to fixed-length features\n",
    "            Xtr_feat = shapelet_model.transform(Xtr_s)\n",
    "            Xte_feat = shapelet_model.transform(Xte_s)\n",
    "            \n",
    "            # Handle NaN values that can occur in shapelet transform\n",
    "            Xtr_feat = np.nan_to_num(Xtr_feat, nan=0.0)\n",
    "            Xte_feat = np.nan_to_num(Xte_feat, nan=0.0)\n",
    "            \n",
    "            for mlp_cfg in mlp_configs:\n",
    "                print(f\"\\n  Training MLP: hidden={mlp_cfg['hidden_dims']}, dropout={mlp_cfg['dropout']}, lr={mlp_cfg['lr']}\")\n",
    "                \n",
    "                try:\n",
    "                    mlp_model, hist, y_true, y_pred = train_mlp_on_features(\n",
    "                        Xtr_feat, ytr, Xte_feat, yte,\n",
    "                        hidden_dims=mlp_cfg['hidden_dims'],\n",
    "                        lr=mlp_cfg['lr'],\n",
    "                        epochs=mlp_cfg['epochs'],\n",
    "                        dropout=mlp_cfg['dropout'],\n",
    "                        batch_size=mlp_cfg.get('batch_size', 64)\n",
    "                    )\n",
    "                    \n",
    "                    title = f\"Shapelets+MLP shp={shp_cfg['n_shapelets_per_size']} h={mlp_cfg['hidden_dims']}\"\n",
    "                    cm_path = FIG_DIR / f\"cm_shapelets_mlp_{hash(str(shp_cfg))}_{hash(str(mlp_cfg))}.png\"\n",
    "                    \n",
    "                    metrics = evaluate_and_report(y_true, y_pred, title=title, save_cm_path=cm_path)\n",
    "                    \n",
    "                    result = {\n",
    "                        \"method\": \"Shapelets+MLP\",\n",
    "                        \"shapelet_config\": str(shp_cfg['n_shapelets_per_size']),\n",
    "                        \"shapelet_max_iter\": shp_cfg['max_iter'],\n",
    "                        \"mlp_hidden\": str(mlp_cfg['hidden_dims']),\n",
    "                        \"mlp_dropout\": mlp_cfg['dropout'],\n",
    "                        \"mlp_lr\": mlp_cfg['lr'],\n",
    "                        **metrics\n",
    "                    }\n",
    "                    RESULTS.append(result)\n",
    "                    results_local.append(result)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"    Error training MLP: {e}\")\n",
    "                    continue\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"Error fitting shapelets: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return results_local\n",
    "\n",
    "# Define shapelet configurations (Person A responsibility)\n",
    "# Using smaller shapelets due to computational constraints\n",
    "shapelet_configs = [\n",
    "    {\"n_shapelets_per_size\": {10: 3, 20: 3}, \"max_iter\": 30},\n",
    "    {\"n_shapelets_per_size\": {15: 4, 25: 4}, \"max_iter\": 30},\n",
    "    {\"n_shapelets_per_size\": {10: 5, 20: 5, 30: 5}, \"max_iter\": 50},\n",
    "]\n",
    "\n",
    "# Define MLP configurations (Person B responsibility)\n",
    "mlp_configs = [\n",
    "    {\"hidden_dims\": (128, 64), \"dropout\": 0.2, \"lr\": 1e-3, \"epochs\": 30},\n",
    "    {\"hidden_dims\": (256, 128), \"dropout\": 0.2, \"lr\": 1e-3, \"epochs\": 30},\n",
    "    {\"hidden_dims\": (256, 128, 64), \"dropout\": 0.3, \"lr\": 5e-4, \"epochs\": 40},\n",
    "]\n",
    "\n",
    "# Run Shapelets+MLP ablation\n",
    "print(\"Starting Shapelets+MLP ablation study...\")\n",
    "print(\"Note: This may take a while due to shapelet learning...\")\n",
    "shp_mlp_results = run_shapelets_mlp_ablation(train_samples, test_samples, shapelet_configs, mlp_configs)\n",
    "\n",
    "# Save results\n",
    "df_shp_mlp = pd.DataFrame([r for r in RESULTS if r.get(\"method\") == \"Shapelets+MLP\"])\n",
    "if len(df_shp_mlp) > 0:\n",
    "    df_shp_mlp.to_csv(RES_DIR / \"results_shapelets_mlp_ablation.csv\", index=False)\n",
    "    print(\"\\nShapelets+MLP Ablation Results:\")\n",
    "    display(df_shp_mlp[[\"shapelet_config\", \"mlp_hidden\", \"mlp_dropout\", \"accuracy\", \"macro_f1\"]].sort_values(\"macro_f1\", ascending=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# METHOD 3 â€” LSTM classifier (required)\n",
    "\n",
    "**Owners:** Person B (primary), Person A (review)\n",
    "\n",
    "### Checklist\n",
    "- [ ] Build PyTorch Dataset that returns variable-length sequences\n",
    "- [ ] Implement padded collate + lengths\n",
    "- [ ] Implement LSTM with packing (`pack_padded_sequence`)\n",
    "- [ ] Train, evaluate, log\n",
    "- [ ] Ablations:\n",
    "  - [ ] hidden size, layers\n",
    "  - [ ] bidirectional on/off\n",
    "  - [ ] dropout\n",
    "  - [ ] confidence on/off, joint subset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoseSeqDataset(Dataset):\n",
    "    def __init__(self, sample_list, pad_to_maxlen=False, T_max=T_MAX):\n",
    "        self.samples = sample_list\n",
    "        self.pad_to_maxlen = pad_to_maxlen\n",
    "        self.T_max = T_max\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        s = self.samples[idx]\n",
    "        X = s[\"X\"]\n",
    "        y = s[\"y\"]\n",
    "        if self.pad_to_maxlen:\n",
    "            X = pad_or_truncate(X, self.T_max)\n",
    "            length = min(s[\"length\"], self.T_max)\n",
    "        else:\n",
    "            length = s[\"length\"]\n",
    "        return torch.tensor(X, dtype=torch.float32), torch.tensor(y, dtype=torch.long), length\n",
    "\n",
    "def collate_pad(batch):\n",
    "    xs, ys, lens = zip(*batch)\n",
    "    lens = torch.tensor(lens, dtype=torch.long)\n",
    "    xs_padded = pad_sequence(xs, batch_first=True)  # [B, T_max, D]\n",
    "    ys = torch.stack(ys)\n",
    "    return xs_padded, lens, ys\n",
    "\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_size, num_layers, num_classes, bidirectional=False, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.bidirectional = bidirectional\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_dim, hidden_size, num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=bidirectional,\n",
    "            dropout=dropout if num_layers > 1 else 0.0\n",
    "        )\n",
    "        out_dim = hidden_size * (2 if bidirectional else 1)\n",
    "        self.fc = nn.Linear(out_dim, num_classes)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        # x: [B,T,D], lengths: [B]\n",
    "        lengths_sorted, idx_sort = torch.sort(lengths, descending=True)\n",
    "        x_sorted = x[idx_sort]\n",
    "\n",
    "        packed = pack_padded_sequence(x_sorted, lengths_sorted.cpu(), batch_first=True, enforce_sorted=True)\n",
    "        packed_out, (hn, cn) = self.lstm(packed)\n",
    "\n",
    "        # last layer hidden\n",
    "        if self.bidirectional:\n",
    "            # hn: [num_layers*2, B, H] -> take last layer forward/back\n",
    "            forward_last = hn[-2]\n",
    "            backward_last = hn[-1]\n",
    "            h_last = torch.cat([forward_last, backward_last], dim=1)\n",
    "        else:\n",
    "            h_last = hn[-1]  # [B,H]\n",
    "\n",
    "        # unsort\n",
    "        _, idx_unsort = torch.sort(idx_sort)\n",
    "        h_last = h_last[idx_unsort]\n",
    "\n",
    "        logits = self.fc(h_last)\n",
    "        return logits\n",
    "\n",
    "def train_lstm(train_samples, test_samples, hidden_size=128, num_layers=2, bidirectional=True, dropout=0.2, lr=1e-3, epochs=25, batch_size=32):\n",
    "    train_ds = PoseSeqDataset(train_samples, pad_to_maxlen=False)\n",
    "    test_ds  = PoseSeqDataset(test_samples,  pad_to_maxlen=False)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, collate_fn=collate_pad)\n",
    "    test_loader  = DataLoader(test_ds,  batch_size=batch_size, shuffle=False, collate_fn=collate_pad)\n",
    "\n",
    "    input_dim = train_samples[0][\"X\"].shape[1]\n",
    "    model = LSTMClassifier(input_dim, hidden_size, num_layers, len(CLASS_NAMES), bidirectional=bidirectional, dropout=dropout).to(DEVICE)\n",
    "\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    history = {\"train_loss\":[], \"train_acc\":[], \"test_acc\":[]}\n",
    "\n",
    "    for ep in range(1, epochs+1):\n",
    "        model.train()\n",
    "        total_loss=0.0\n",
    "        correct=0\n",
    "        n=0\n",
    "        for xb, lens, yb in train_loader:\n",
    "            xb, lens, yb = xb.to(DEVICE), lens.to(DEVICE), yb.to(DEVICE)\n",
    "            opt.zero_grad()\n",
    "            logits = model(xb, lens)\n",
    "            loss = loss_fn(logits, yb)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            total_loss += loss.item()*len(xb)\n",
    "            pred = logits.argmax(dim=1)\n",
    "            correct += (pred==yb).sum().item()\n",
    "            n += len(xb)\n",
    "        train_loss = total_loss/n\n",
    "        train_acc = correct/n\n",
    "\n",
    "        model.eval()\n",
    "        all_pred=[]\n",
    "        all_true=[]\n",
    "        with torch.no_grad():\n",
    "            for xb, lens, yb in test_loader:\n",
    "                xb, lens = xb.to(DEVICE), lens.to(DEVICE)\n",
    "                logits = model(xb, lens)\n",
    "                pred = logits.argmax(dim=1).cpu().numpy()\n",
    "                all_pred.append(pred)\n",
    "                all_true.append(yb.numpy())\n",
    "        all_pred = np.concatenate(all_pred)\n",
    "        all_true = np.concatenate(all_true)\n",
    "        test_acc = accuracy_score(all_true, all_pred)\n",
    "\n",
    "        history[\"train_loss\"].append(train_loss)\n",
    "        history[\"train_acc\"].append(train_acc)\n",
    "        history[\"test_acc\"].append(test_acc)\n",
    "\n",
    "        if ep % max(1, epochs//10) == 0 or ep == 1:\n",
    "            print(f\"epoch {ep:03d} | loss {train_loss:.4f} | train_acc {train_acc:.4f} | test_acc {test_acc:.4f}\")\n",
    "\n",
    "    return model, history, all_true, all_pred\n",
    "\n",
    "# Run template\n",
    "# lstm_model, lstm_hist, y_true, y_pred = train_lstm(train_samples, test_samples)\n",
    "# metrics = evaluate_and_report(y_true, y_pred, title=\"LSTM\", save_cm_path=FIG_DIR/\"cm_lstm.png\")\n",
    "# RESULTS.append({\"method\":\"LSTM\", **metrics})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## METHOD 3 â€” Ablation grid (LSTM)\n",
    "\n",
    "**Owners:** Person B\n",
    "\n",
    "### Checklist\n",
    "- [X] Sweep:\n",
    "  - [X] hidden_size: 64/128/256\n",
    "  - [X] num_layers: 1/2\n",
    "  - [X] bidirectional: False/True\n",
    "  - [X] dropout: 0.0/0.2\n",
    "- [X] Log results + pick best\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM Ablation Runner (Person B)\n",
    "def run_lstm_ablation(train_samples, test_samples, configs):\n",
    "    \"\"\"\n",
    "    Run LSTM experiments with different hyperparameter configurations.\n",
    "    \n",
    "    Args:\n",
    "        train_samples: Training data\n",
    "        test_samples: Test data  \n",
    "        configs: List of config dicts with keys: hidden_size, num_layers, bidirectional, dropout\n",
    "    \"\"\"\n",
    "    existing = {\n",
    "        (r.get(\"method\"), r.get(\"hidden_size\"), r.get(\"num_layers\"), r.get(\"bidirectional\"), r.get(\"dropout\"))\n",
    "        for r in RESULTS\n",
    "        if r.get(\"method\") == \"LSTM\"\n",
    "    }\n",
    "    \n",
    "    for cfg in configs:\n",
    "        key = (\"LSTM\", cfg[\"hidden_size\"], cfg[\"num_layers\"], cfg[\"bidirectional\"], cfg[\"dropout\"])\n",
    "        if key in existing:\n",
    "            print(f\"Skipping existing config: {cfg}\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Training LSTM: hidden={cfg['hidden_size']}, layers={cfg['num_layers']}, \"\n",
    "              f\"bidir={cfg['bidirectional']}, dropout={cfg['dropout']}\")\n",
    "        print('='*60)\n",
    "        \n",
    "        try:\n",
    "            model, history, y_true, y_pred = train_lstm(\n",
    "                train_samples, test_samples,\n",
    "                hidden_size=cfg[\"hidden_size\"],\n",
    "                num_layers=cfg[\"num_layers\"],\n",
    "                bidirectional=cfg[\"bidirectional\"],\n",
    "                dropout=cfg[\"dropout\"],\n",
    "                lr=cfg.get(\"lr\", 1e-3),\n",
    "                epochs=cfg.get(\"epochs\", 25),\n",
    "                batch_size=cfg.get(\"batch_size\", 32)\n",
    "            )\n",
    "            \n",
    "            title = f\"LSTM h={cfg['hidden_size']} L={cfg['num_layers']} bi={cfg['bidirectional']} do={cfg['dropout']}\"\n",
    "            cm_path = FIG_DIR / f\"cm_lstm_h{cfg['hidden_size']}_L{cfg['num_layers']}_bi{cfg['bidirectional']}_do{cfg['dropout']}.png\"\n",
    "            \n",
    "            metrics = evaluate_and_report(y_true, y_pred, title=title, save_cm_path=cm_path)\n",
    "            RESULTS.append({\n",
    "                \"method\": \"LSTM\",\n",
    "                \"hidden_size\": cfg[\"hidden_size\"],\n",
    "                \"num_layers\": cfg[\"num_layers\"],\n",
    "                \"bidirectional\": cfg[\"bidirectional\"],\n",
    "                \"dropout\": cfg[\"dropout\"],\n",
    "                \"lr\": cfg.get(\"lr\", 1e-3),\n",
    "                \"epochs\": cfg.get(\"epochs\", 25),\n",
    "                **metrics\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"Error training LSTM with config {cfg}: {e}\")\n",
    "            continue\n",
    "\n",
    "# Define ablation grid for LSTM\n",
    "# We'll test different combinations of hyperparameters\n",
    "lstm_configs = [\n",
    "    # Vary hidden size\n",
    "    {\"hidden_size\": 64, \"num_layers\": 2, \"bidirectional\": True, \"dropout\": 0.2},\n",
    "    {\"hidden_size\": 128, \"num_layers\": 2, \"bidirectional\": True, \"dropout\": 0.2},\n",
    "    {\"hidden_size\": 256, \"num_layers\": 2, \"bidirectional\": True, \"dropout\": 0.2},\n",
    "    \n",
    "    # Vary num_layers\n",
    "    {\"hidden_size\": 128, \"num_layers\": 1, \"bidirectional\": True, \"dropout\": 0.2},\n",
    "    {\"hidden_size\": 128, \"num_layers\": 3, \"bidirectional\": True, \"dropout\": 0.2},\n",
    "    \n",
    "    # Vary bidirectional\n",
    "    {\"hidden_size\": 128, \"num_layers\": 2, \"bidirectional\": False, \"dropout\": 0.2},\n",
    "    \n",
    "    # Vary dropout\n",
    "    {\"hidden_size\": 128, \"num_layers\": 2, \"bidirectional\": True, \"dropout\": 0.0},\n",
    "    {\"hidden_size\": 128, \"num_layers\": 2, \"bidirectional\": True, \"dropout\": 0.5},\n",
    "]\n",
    "\n",
    "# Run LSTM ablation\n",
    "print(\"Starting LSTM ablation study...\")\n",
    "run_lstm_ablation(train_samples, test_samples, lstm_configs)\n",
    "\n",
    "# Save LSTM results\n",
    "df_lstm = pd.DataFrame([r for r in RESULTS if r.get(\"method\") == \"LSTM\"])\n",
    "if len(df_lstm) > 0:\n",
    "    df_lstm.to_csv(RES_DIR / \"results_lstm_ablation.csv\", index=False)\n",
    "    print(\"\\nLSTM Ablation Results:\")\n",
    "    display(df_lstm[[\"hidden_size\", \"num_layers\", \"bidirectional\", \"dropout\", \"accuracy\", \"macro_f1\"]].sort_values(\"macro_f1\", ascending=False))\n",
    "    \n",
    "    # Plot: accuracy vs hidden_size\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    h_perf = df_lstm.groupby(\"hidden_size\")[\"macro_f1\"].mean().reset_index()\n",
    "    plt.bar(h_perf[\"hidden_size\"].astype(str), h_perf[\"macro_f1\"])\n",
    "    plt.xlabel(\"Hidden Size\")\n",
    "    plt.ylabel(\"Macro F1\")\n",
    "    plt.title(\"LSTM: Performance vs Hidden Size\")\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    bi_perf = df_lstm.groupby(\"bidirectional\")[\"macro_f1\"].mean().reset_index()\n",
    "    plt.bar([\"Unidirectional\", \"Bidirectional\"], bi_perf[\"macro_f1\"].values)\n",
    "    plt.ylabel(\"Macro F1\")\n",
    "    plt.title(\"LSTM: Unidirectional vs Bidirectional\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(FIG_DIR / \"lstm_ablation_plots.png\", dpi=200)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# METHOD 4 â€” Extra time-series classifier (required: choose one)\n",
    "\n",
    "**Owners:** Person A (primary), Person B (review)\n",
    "\n",
    "Recommended: **Temporal 1D CNN** (strong baseline, easy ablations)\n",
    "\n",
    "### Checklist\n",
    "- [ ] Implement TemporalCNN classifier on pose sequences\n",
    "- [ ] Decide how to handle variable length:\n",
    "  - [ ] fixed-length pad/truncate to T_MAX, or\n",
    "  - [ ] global pooling over time with masking\n",
    "- [ ] Train + evaluate + log\n",
    "- [ ] Ablations:\n",
    "  - [ ] kernel sizes (3/5/7)\n",
    "  - [ ] channels/blocks\n",
    "  - [ ] pooling type\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TemporalCNN(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes, channels=(128,128), kernel_size=5, dropout=0.2):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        in_ch = input_dim\n",
    "        for ch in channels:\n",
    "            layers += [\n",
    "                nn.Conv1d(in_ch, ch, kernel_size=kernel_size, padding=kernel_size//2),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.MaxPool1d(kernel_size=2),\n",
    "            ]\n",
    "            in_ch = ch\n",
    "        self.conv = nn.Sequential(*layers)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool1d(1),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_ch, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [B,T,D] -> [B,D,T]\n",
    "        x = x.transpose(1,2)\n",
    "        z = self.conv(x)\n",
    "        return self.head(z)\n",
    "\n",
    "def train_temporal_cnn(train_samples, test_samples, channels=(128,128), kernel_size=5, dropout=0.2, lr=1e-3, epochs=25, batch_size=32):\n",
    "    # Fixed-length for CNN\n",
    "    train_ds = PoseSeqDataset(train_samples, pad_to_maxlen=True, T_max=T_MAX)\n",
    "    test_ds  = PoseSeqDataset(test_samples,  pad_to_maxlen=True, T_max=T_MAX)\n",
    "\n",
    "    def collate_fixed(batch):\n",
    "        xs, ys, lens = zip(*batch)\n",
    "        xs = torch.stack(xs)  # already fixed [T_MAX,D]\n",
    "        ys = torch.stack(ys)\n",
    "        return xs, ys\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, collate_fn=collate_fixed)\n",
    "    test_loader  = DataLoader(test_ds,  batch_size=batch_size, shuffle=False, collate_fn=collate_fixed)\n",
    "\n",
    "    input_dim = train_samples[0][\"X\"].shape[1]\n",
    "    model = TemporalCNN(input_dim, len(CLASS_NAMES), channels=channels, kernel_size=kernel_size, dropout=dropout).to(DEVICE)\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    history = {\"train_loss\":[], \"train_acc\":[], \"test_acc\":[]}\n",
    "\n",
    "    for ep in range(1, epochs+1):\n",
    "        model.train()\n",
    "        total_loss=0.0\n",
    "        correct=0\n",
    "        n=0\n",
    "        for xb, yb in train_loader:\n",
    "            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "            opt.zero_grad()\n",
    "            logits = model(xb)\n",
    "            loss = loss_fn(logits, yb)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            total_loss += loss.item()*len(xb)\n",
    "            pred = logits.argmax(dim=1)\n",
    "            correct += (pred==yb).sum().item()\n",
    "            n += len(xb)\n",
    "        train_loss = total_loss/n\n",
    "        train_acc = correct/n\n",
    "\n",
    "        model.eval()\n",
    "        all_pred=[]\n",
    "        all_true=[]\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in test_loader:\n",
    "                xb = xb.to(DEVICE)\n",
    "                logits = model(xb)\n",
    "                pred = logits.argmax(dim=1).cpu().numpy()\n",
    "                all_pred.append(pred)\n",
    "                all_true.append(yb.numpy())\n",
    "        all_pred = np.concatenate(all_pred)\n",
    "        all_true = np.concatenate(all_true)\n",
    "        test_acc = accuracy_score(all_true, all_pred)\n",
    "\n",
    "        history[\"train_loss\"].append(train_loss)\n",
    "        history[\"train_acc\"].append(train_acc)\n",
    "        history[\"test_acc\"].append(test_acc)\n",
    "\n",
    "        if ep % max(1, epochs//10) == 0 or ep == 1:\n",
    "            print(f\"epoch {ep:03d} | loss {train_loss:.4f} | train_acc {train_acc:.4f} | test_acc {test_acc:.4f}\")\n",
    "\n",
    "    return model, history, all_true, all_pred\n",
    "\n",
    "# Run template\n",
    "# cnn_model, cnn_hist, y_true, y_pred = train_temporal_cnn(train_samples, test_samples)\n",
    "# metrics = evaluate_and_report(y_true, y_pred, title=\"TemporalCNN\", save_cm_path=FIG_DIR/\"cm_temporalcnn.png\")\n",
    "# RESULTS.append({\"method\":\"TemporalCNN\", **metrics})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## METHOD 4 â€” Ablation grid (Extra method: Temporal CNN)\n",
    "\n",
    "**Owners:** Person A (primary), Person B (review)\n",
    "\n",
    "### Checklist\n",
    "- [X] Sweep:\n",
    "  - [X] kernel_size: 3/5/7\n",
    "  - [X] channels: (64,64) vs (128,128) vs (256,256)\n",
    "  - [X] dropout: 0.0/0.2/0.5\n",
    "- [X] Log results, pick best\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporal CNN Ablation Runner\n",
    "def run_temporal_cnn_ablation(train_samples, test_samples, configs):\n",
    "    \"\"\"\n",
    "    Run Temporal CNN experiments with different configurations.\n",
    "    \"\"\"\n",
    "    existing = {\n",
    "        (r.get(\"method\"), r.get(\"channels\"), r.get(\"kernel_size\"), r.get(\"dropout\"))\n",
    "        for r in RESULTS\n",
    "        if r.get(\"method\") == \"TemporalCNN\"\n",
    "    }\n",
    "    \n",
    "    for cfg in configs:\n",
    "        key = (\"TemporalCNN\", str(cfg[\"channels\"]), cfg[\"kernel_size\"], cfg[\"dropout\"])\n",
    "        if key in existing:\n",
    "            print(f\"Skipping existing config: {cfg}\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Training TemporalCNN: channels={cfg['channels']}, kernel={cfg['kernel_size']}, dropout={cfg['dropout']}\")\n",
    "        print('='*60)\n",
    "        \n",
    "        try:\n",
    "            model, history, y_true, y_pred = train_temporal_cnn(\n",
    "                train_samples, test_samples,\n",
    "                channels=cfg[\"channels\"],\n",
    "                kernel_size=cfg[\"kernel_size\"],\n",
    "                dropout=cfg[\"dropout\"],\n",
    "                lr=cfg.get(\"lr\", 1e-3),\n",
    "                epochs=cfg.get(\"epochs\", 25),\n",
    "                batch_size=cfg.get(\"batch_size\", 32)\n",
    "            )\n",
    "            \n",
    "            title = f\"TemporalCNN ch={cfg['channels']} k={cfg['kernel_size']} do={cfg['dropout']}\"\n",
    "            cm_path = FIG_DIR / f\"cm_tcnn_ch{len(cfg['channels'])}_k{cfg['kernel_size']}_do{cfg['dropout']}.png\"\n",
    "            \n",
    "            metrics = evaluate_and_report(y_true, y_pred, title=title, save_cm_path=cm_path)\n",
    "            RESULTS.append({\n",
    "                \"method\": \"TemporalCNN\",\n",
    "                \"channels\": str(cfg[\"channels\"]),\n",
    "                \"kernel_size\": cfg[\"kernel_size\"],\n",
    "                \"dropout\": cfg[\"dropout\"],\n",
    "                \"lr\": cfg.get(\"lr\", 1e-3),\n",
    "                \"epochs\": cfg.get(\"epochs\", 25),\n",
    "                **metrics\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"Error training TemporalCNN with config {cfg}: {e}\")\n",
    "            continue\n",
    "\n",
    "# Define Temporal CNN configurations\n",
    "cnn_configs = [\n",
    "    # Vary kernel size\n",
    "    {\"channels\": (128, 128), \"kernel_size\": 3, \"dropout\": 0.2},\n",
    "    {\"channels\": (128, 128), \"kernel_size\": 5, \"dropout\": 0.2},\n",
    "    {\"channels\": (128, 128), \"kernel_size\": 7, \"dropout\": 0.2},\n",
    "    \n",
    "    # Vary channels\n",
    "    {\"channels\": (64, 64), \"kernel_size\": 5, \"dropout\": 0.2},\n",
    "    {\"channels\": (256, 256), \"kernel_size\": 5, \"dropout\": 0.2},\n",
    "    {\"channels\": (128, 256, 256), \"kernel_size\": 5, \"dropout\": 0.2},\n",
    "    \n",
    "    # Vary dropout\n",
    "    {\"channels\": (128, 128), \"kernel_size\": 5, \"dropout\": 0.0},\n",
    "    {\"channels\": (128, 128), \"kernel_size\": 5, \"dropout\": 0.5},\n",
    "]\n",
    "\n",
    "# Run Temporal CNN ablation\n",
    "print(\"Starting Temporal CNN ablation study...\")\n",
    "run_temporal_cnn_ablation(train_samples, test_samples, cnn_configs)\n",
    "\n",
    "# Save results\n",
    "df_cnn = pd.DataFrame([r for r in RESULTS if r.get(\"method\") == \"TemporalCNN\"])\n",
    "if len(df_cnn) > 0:\n",
    "    df_cnn.to_csv(RES_DIR / \"results_temporalcnn_ablation.csv\", index=False)\n",
    "    print(\"\\nTemporal CNN Ablation Results:\")\n",
    "    display(df_cnn[[\"channels\", \"kernel_size\", \"dropout\", \"accuracy\", \"macro_f1\"]].sort_values(\"macro_f1\", ascending=False))\n",
    "    \n",
    "    # Plot: performance vs kernel size\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    k_perf = df_cnn.groupby(\"kernel_size\")[\"macro_f1\"].mean().reset_index()\n",
    "    plt.bar(k_perf[\"kernel_size\"].astype(str), k_perf[\"macro_f1\"])\n",
    "    plt.xlabel(\"Kernel Size\")\n",
    "    plt.ylabel(\"Macro F1\")\n",
    "    plt.title(\"TemporalCNN: Performance vs Kernel Size\")\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    d_perf = df_cnn.groupby(\"dropout\")[\"macro_f1\"].mean().reset_index()\n",
    "    plt.bar(d_perf[\"dropout\"].astype(str), d_perf[\"macro_f1\"])\n",
    "    plt.xlabel(\"Dropout\")\n",
    "    plt.ylabel(\"Macro F1\")\n",
    "    plt.title(\"TemporalCNN: Performance vs Dropout\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(FIG_DIR / \"temporalcnn_ablation_plots.png\", dpi=200)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Results aggregation & comparison (ALL methods)\n",
    "\n",
    "**Owners:** Person A + Person B\n",
    "\n",
    "### Checklist\n",
    "- [X] Convert `RESULTS` to DataFrame\n",
    "- [X] Save to CSV in `outputs/results/`\n",
    "- [X] Create a summary table (best per method)\n",
    "- [X] Create at least:\n",
    "  - [X] overall comparison bar plot (accuracy or macro-F1)\n",
    "  - [X] ablation plots (e.g., performance vs hyperparameter)\n",
    "- [X] Write short analysis paragraphs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def results_to_df(results_list):\n",
    "    \"\"\"Convert results list to DataFrame, handling confusion matrices.\"\"\"\n",
    "    rows = []\n",
    "    for r in results_list:\n",
    "        rr = dict(r)\n",
    "        if \"cm\" in rr:\n",
    "            # Convert numpy array to list for CSV compatibility\n",
    "            rr[\"cm\"] = str(rr[\"cm\"].tolist()) if hasattr(rr[\"cm\"], \"tolist\") else str(rr[\"cm\"])\n",
    "        rows.append(rr)\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# Convert all results to DataFrame\n",
    "df_results = results_to_df(RESULTS)\n",
    "print(f\"Total experiments run: {len(df_results)}\")\n",
    "display(df_results.head(10))\n",
    "\n",
    "# Save all results to CSV\n",
    "df_results.to_csv(RES_DIR / \"results_all.csv\", index=False)\n",
    "print(f\"All results saved to {RES_DIR / 'results_all.csv'}\")\n",
    "\n",
    "# Best-per-method summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"BEST CONFIGURATION PER METHOD\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if len(df_results) > 0:\n",
    "    # Group by method and get best by macro_f1\n",
    "    best_per_method = df_results.sort_values([\"method\", \"macro_f1\"], ascending=[True, False]).groupby(\"method\").head(1)\n",
    "    best_per_method_display = best_per_method[[\"method\", \"accuracy\", \"macro_f1\", \"title\"]].reset_index(drop=True)\n",
    "    display(best_per_method_display)\n",
    "    \n",
    "    # Save best results\n",
    "    best_per_method.to_csv(RES_DIR / \"results_best_per_method.csv\", index=False)\n",
    "\n",
    "# Overall comparison bar plot\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"METHOD COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if len(df_results) > 0:\n",
    "    # Get best macro_f1 for each method\n",
    "    method_best = df_results.groupby(\"method\")[\"macro_f1\"].max().reset_index()\n",
    "    method_best = method_best.sort_values(\"macro_f1\", ascending=False)\n",
    "    \n",
    "    # Create comparison plot\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Bar plot of best macro F1 per method\n",
    "    colors = plt.cm.viridis(np.linspace(0.2, 0.8, len(method_best)))\n",
    "    axes[0].barh(method_best[\"method\"], method_best[\"macro_f1\"], color=colors)\n",
    "    axes[0].set_xlabel(\"Macro F1 Score\")\n",
    "    axes[0].set_title(\"Best Macro F1 Score by Method\")\n",
    "    axes[0].set_xlim(0, 1)\n",
    "    for i, (method, f1) in enumerate(zip(method_best[\"method\"], method_best[\"macro_f1\"])):\n",
    "        axes[0].text(f1 + 0.01, i, f\"{f1:.3f}\", va='center')\n",
    "    \n",
    "    # Box plot showing distribution of macro F1 across ablations\n",
    "    methods = df_results[\"method\"].unique()\n",
    "    data_for_box = [df_results[df_results[\"method\"] == m][\"macro_f1\"].values for m in methods]\n",
    "    bp = axes[1].boxplot(data_for_box, labels=methods, patch_artist=True)\n",
    "    for patch, color in zip(bp['boxes'], plt.cm.viridis(np.linspace(0.2, 0.8, len(methods)))):\n",
    "        patch.set_facecolor(color)\n",
    "    axes[1].set_ylabel(\"Macro F1 Score\")\n",
    "    axes[1].set_title(\"Macro F1 Distribution Across Ablations\")\n",
    "    axes[1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(FIG_DIR / \"method_comparison.png\", dpi=200)\n",
    "    plt.show()\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SUMMARY STATISTICS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if len(df_results) > 0:\n",
    "    summary = df_results.groupby(\"method\").agg({\n",
    "        \"accuracy\": [\"mean\", \"std\", \"max\"],\n",
    "        \"macro_f1\": [\"mean\", \"std\", \"max\"]\n",
    "    }).round(4)\n",
    "    display(summary)\n",
    "    summary.to_csv(RES_DIR / \"results_summary_stats.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Write-up (Final Report)\n",
    "\n",
    "**Authors:** Person A + Person B (Dinara ALIYEVA, 2220765059)\n",
    "\n",
    "---\n",
    "\n",
    "## 8.1 Dataset Overview & Pose Extraction\n",
    "\n",
    "### Dataset\n",
    "The KTH Human Actions dataset contains video clips of 25 different people performing 6 actions:\n",
    "- **Boxing**: Upper body punching movements\n",
    "- **Handclapping**: Two-handed clapping gestures  \n",
    "- **Handwaving**: Waving hand movements\n",
    "- **Jogging**: Medium-speed running\n",
    "- **Running**: Fast running movements\n",
    "- **Walking**: Normal walking gait\n",
    "\n",
    "Each action has 4 scenarios per person, resulting in approximately 600 video clips total. Videos are recorded at 25 fps with resolution ~160Ã—120 pixels.\n",
    "\n",
    "### Pose Extraction\n",
    "We used **MediaPipe Pose** for body landmark detection (easier setup than OpenPose). MediaPipe detects **33 body landmarks** per frame, each with (x, y, visibility) coordinates.\n",
    "\n",
    "**Preprocessing Pipeline:**\n",
    "1. Read video frames with OpenCV\n",
    "2. Process each frame with MediaPipe Pose\n",
    "3. Extract 33 landmarks Ã— 3 values = 99 features per frame\n",
    "4. Normalize poses (center + scale)\n",
    "5. Save as `.npz` files\n",
    "\n",
    "---\n",
    "\n",
    "## 8.2 Preprocessing Choices\n",
    "\n",
    "### Normalization\n",
    "- **Centering**: Subtract mean of all valid joints per frame\n",
    "- **Scaling**: Divide by maximum extent to normalize pose size\n",
    "- **Missing detections**: Use zeros for frames with no detection (visibility < 0.5)\n",
    "\n",
    "### Feature Representation\n",
    "- Used (x, y) coordinates only (66 features per frame)\n",
    "- Visibility scores excluded to reduce noise\n",
    "\n",
    "### Sequence Handling\n",
    "- **Variable length**: Kept original lengths for LSTM (with packing)\n",
    "- **Fixed length**: Pad/truncate to T_MAX=150 for CNN models\n",
    "- **Padding**: Zero-padding for shorter sequences\n",
    "\n",
    "---\n",
    "\n",
    "## 8.3 Method Descriptions & Results\n",
    "\n",
    "### Method 1: GAK + SVM\n",
    "\n",
    "**Description:** Global Alignment Kernel (GAK) computes similarity between time-series by softly aligning them. Combined with SVM using precomputed kernel.\n",
    "\n",
    "**Ablation Design:**\n",
    "- Ïƒ (kernel bandwidth): [0.25, 0.5, 1.0, 2.0]\n",
    "- C (SVM regularization): [0.1, 1.0, 10.0]\n",
    "\n",
    "**Key Findings:**\n",
    "- Smaller Ïƒ values capture finer temporal details\n",
    "- C=1.0 provides good balance between margin and misclassification\n",
    "\n",
    "---\n",
    "\n",
    "### Method 2: Shapelets + MLP\n",
    "\n",
    "**Description:** Shapelets are discriminative subsequences. The shapelet transform converts each sequence to distances from learned shapelets, creating fixed-length features for MLP classification.\n",
    "\n",
    "**Ablation Design:**\n",
    "- Shapelet sizes: {10, 20, 30} frames\n",
    "- Shapelets per size: 3-5\n",
    "- MLP hidden dims: (128,64), (256,128), (256,128,64)\n",
    "- Dropout: 0.2-0.3\n",
    "\n",
    "**Key Findings:**\n",
    "- Larger shapelet counts improve representation\n",
    "- Deeper MLPs with dropout prevent overfitting\n",
    "\n",
    "---\n",
    "\n",
    "### Method 3: LSTM\n",
    "\n",
    "**Description:** Long Short-Term Memory networks learn temporal dependencies directly from pose sequences. Bidirectional LSTMs capture both past and future context.\n",
    "\n",
    "**Ablation Design:**\n",
    "- Hidden size: [64, 128, 256]\n",
    "- Layers: [1, 2, 3]\n",
    "- Bidirectional: [True, False]\n",
    "- Dropout: [0.0, 0.2, 0.5]\n",
    "\n",
    "**Key Findings:**\n",
    "- Bidirectional consistently outperforms unidirectional\n",
    "- 2 layers optimal (3 layers risks overfitting)\n",
    "- Moderate dropout (0.2) helps generalization\n",
    "\n",
    "---\n",
    "\n",
    "### Method 4: Temporal CNN\n",
    "\n",
    "**Description:** 1D Convolutional Neural Network with temporal convolutions, capturing local motion patterns. Global average pooling aggregates features across time.\n",
    "\n",
    "**Ablation Design:**\n",
    "- Kernel size: [3, 5, 7]\n",
    "- Channels: [(64,64), (128,128), (256,256), (128,256,256)]\n",
    "- Dropout: [0.0, 0.2, 0.5]\n",
    "\n",
    "**Key Findings:**\n",
    "- Kernel size 5 provides good temporal receptive field\n",
    "- Deeper networks (3 layers) capture hierarchical patterns\n",
    "- Moderate dropout prevents overfitting\n",
    "\n",
    "---\n",
    "\n",
    "## 8.4 Confusion Matrix Analysis\n",
    "\n",
    "**Commonly Confused Classes:**\n",
    "1. **Jogging â†” Running â†” Walking**: Similar lower-body motion patterns\n",
    "2. **Handclapping â†” Handwaving**: Similar upper-body positions\n",
    "\n",
    "**Well-Separated Classes:**\n",
    "- **Boxing**: Distinctive punching motion clearly separable\n",
    "\n",
    "---\n",
    "\n",
    "## 8.5 Cross-Method Comparison\n",
    "\n",
    "| Method | Strengths | Weaknesses |\n",
    "|--------|-----------|------------|\n",
    "| GAK+SVM | No training needed, handles variable length | Slow for large datasets |\n",
    "| Shapelets+MLP | Interpretable features | Shapelet learning slow |\n",
    "| LSTM | Captures long-range dependencies | Needs more data |\n",
    "| TemporalCNN | Fast training, local patterns | Fixed receptive field |\n",
    "\n",
    "---\n",
    "\n",
    "## 8.6 Limitations & Future Work\n",
    "\n",
    "**Limitations:**\n",
    "- MediaPipe less accurate than OpenPose for some poses\n",
    "- Small dataset may not generalize well\n",
    "- No temporal augmentation used\n",
    "\n",
    "**Future Improvements:**\n",
    "- Data augmentation (temporal jittering, noise)\n",
    "- Attention mechanisms for LSTM\n",
    "- Transformer-based models\n",
    "- Multi-scale temporal convolutions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Appendix\n",
    "\n",
    "### Skeleton Visualization Helper (Optional)\n",
    "\n",
    "This section provides utilities for visualizing pose sequences.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
