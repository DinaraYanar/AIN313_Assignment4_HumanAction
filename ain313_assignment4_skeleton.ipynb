{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AIN313 - Assignment 4 (Fall 2025) - Human Action Classification from Pose Time-Series\n",
    "\n",
    "> **Goal:** Build **4** human-action classifiers from **OpenPose BODY-25** pose time-series and compare them with **ablation studies** and **clear plots/tables**.\n",
    "\n",
    "**Due:** Dec 26, 2025 23:59:59\n",
    "\n",
    "**Dataset (summary):**\n",
    "- 6 actions: boxing, handclapping, handwaving, jogging, running, walking\n",
    "- ~160x120 resolution, 25 fps, 25 actors\n",
    "- Download link: use the Google Drive link provided in the PDF\n",
    "\n",
    "**Deliverables (single zip, do not include dataset):**\n",
    "- `project.ipynb` (report + code, self-contained)\n",
    "- `project.py` (exported from notebook)\n",
    "- Name: `project_studentIDs.zip`\n",
    "\n",
    "**Team members (fill in):**\n",
    "- **Person A:** _name, student ID_\n",
    "- **Person B:** _name, student ID_\n",
    "\n",
    "**How to use this notebook**\n",
    "- This is a clean assignment skeleton with assigned owners and TODO checklists.\n",
    "- Keep results reproducible: fix seeds, log configs, save metrics tables/figures.\n",
    "- Each method needs an ablation study (multiple configs), not a single run.\n",
    "\n",
    "---\n",
    "\n",
    "## Global TODO (shared)\n",
    "- [ ] Confirm OpenPose extraction method (CLI or Python bindings)\n",
    "- [ ] Confirm dataset path(s) and label mapping (6 classes)\n",
    "- [ ] Agree on pose representation: `(x,y)` or `(x,y,conf)` and joint subset policy\n",
    "- [ ] Agree on evaluation protocol: stratified split (and/or CV), metrics, ablation grid size\n",
    "- [ ] Decide the extra method (must be course-related and time-series suitable)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Assumptions & constraints (edit these first)\n",
    "\n",
    "**Owners:** Person A + Person B\n",
    "\n",
    "- [ ] Dataset downloaded locally (not committed to GitHub)\n",
    "- [ ] OpenPose BODY-25 available locally (or pose `.npz` already generated)\n",
    "- [ ] `.npz` pose files will be created per video (not committed), containing:\n",
    "  - `pose` (raw keypoints), `pose_norm` (normalized), `frames`, `label`, `video_path`\n",
    "  - optional: `label_name`\n",
    "- [ ] Notebook will run end-to-end assuming `.npz` already exists\n",
    "  (pose extraction cells can be marked as optional if OpenPose is not available on the runner)\n",
    "\n",
    "**Classes (must match dataset folders):**\n",
    "- boxing\n",
    "- handclapping\n",
    "- handwaving\n",
    "- jogging\n",
    "- running\n",
    "- walking\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "# 1) Install / import dependencies\n# Owners: Person A + Person B\n# - [ ] Ensure all required packages are installed in your environment\n# - [ ] Update versions if your course environment requires it\n\nimport os, json, glob, random, math, time\nfrom pathlib import Path\n\nimport numpy as np\nimport pandas as pd\n\nfrom tqdm import tqdm\n\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import (\n    accuracy_score, f1_score, confusion_matrix, classification_report, ConfusionMatrixDisplay\n)\n\n# Time-series + shapelets\n# NOTE: install tslearn if not present: pip install tslearn\ntry:\n    import tslearn\n    from tslearn.utils import to_time_series_dataset\n    from tslearn.preprocessing import TimeSeriesScalerMeanVariance\n    from tslearn.metrics import cdist_gak\nexcept Exception as e:\n    print(\"tslearn import issue:\", e)\n\n# SVM\nfrom sklearn.svm import SVC\n\n# PyTorch (MLP/LSTM/extra)\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1. Configuration & reproducibility\n\n**Owners:** Person B (primary), Person A (review)\n\n### TODO\n- [ ] Set your local paths (dataset, openpose, npz output)\n- [ ] Confirm label mapping is correct for your dataset\n- [ ] Choose pose format:\n  - [ ] `USE_CONFIDENCE = True/False`\n  - [ ] `JOINTS = all or subset`\n- [ ] Decide padding / truncation policy for neural models\n- [ ] Decide evaluation split protocol (default: stratified 80/20)\n"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Configuration (edit paths!)\n",
    "SEED = 42\n",
    "\n",
    "# Dataset link (from PDF)\n",
    "DATASET_URL = \"PASTE_LINK_FROM_PDF\"\n",
    "\n",
    "# Paths (local, not committed)\n",
    "DATASET_ROOT = Path(\"PATH/TO/DATASET\")         # raw videos\n",
    "OPENPOSE_BIN  = Path(\"PATH/TO/OPENPOSE_BIN\")   # optional\n",
    "NPZ_ROOT      = Path(\"PATH/TO/POSE_NPZ\")       # output .npz per video\n",
    "\n",
    "# Output artifacts (OK to commit if small)\n",
    "OUT_DIR = Path(\"outputs\")\n",
    "FIG_DIR = OUT_DIR / \"figures\"\n",
    "RES_DIR = OUT_DIR / \"results\"\n",
    "OUT_DIR.mkdir(exist_ok=True)\n",
    "FIG_DIR.mkdir(exist_ok=True, parents=True)\n",
    "RES_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# Dataset labels\n",
    "CLASS_NAMES = [\"boxing\", \"handclapping\", \"handwaving\", \"jogging\", \"running\", \"walking\"]\n",
    "LABEL2ID = {c:i for i,c in enumerate(CLASS_NAMES)}\n",
    "ID2LABEL = {i:c for c,i in LABEL2ID.items()}\n",
    "\n",
    "# Pose representation\n",
    "USE_CONFIDENCE = False          # True -> include conf channel as feature dim\n",
    "USE_JOINT_SUBSET = False        # True -> only some joints\n",
    "JOINT_IDS = list(range(25))     # BODY-25; replace if subset\n",
    "\n",
    "# Sequence handling\n",
    "PAD_TO_MAXLEN = True            # for neural models\n",
    "T_MAX = 150                     # truncate/pad length if PAD_TO_MAXLEN\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"DEVICE:\", DEVICE)\n",
    "\n",
    "def set_seed(seed=SEED):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(SEED)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2. Data discovery (videos) → metadata table\n\n**Owners:** Person A (primary), Person B (review)\n\n### TODO\n- [ ] Implement video listing (glob patterns depend on dataset)\n- [ ] Confirm label parsing from paths\n- [ ] Build a dataframe: `video_path`, `label_name`, `label_id`, `video_id`\n- [ ] Print dataset summary (counts per class)\n"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "def find_videos(dataset_root: Path, exts=(\".avi\", \".mp4\", \".mov\", \".mkv\")):\n    # TODO (Person A): adjust patterns for dataset structure\n    paths = []\n    for ext in exts:\n        paths.extend(dataset_root.rglob(f\"*{ext}\"))\n    return sorted(paths)\n\ndef infer_label_from_path(video_path: Path):\n    # TODO (Person A): ensure this matches dataset folder naming\n    parts = [p.lower() for p in video_path.parts]\n    for cname in CLASS_NAMES:\n        if cname in parts:\n            return cname\n    return None\n\nvideos = find_videos(DATASET_ROOT)\nrows = []\nfor vp in videos:\n    lbl = infer_label_from_path(vp)\n    if lbl is None:\n        continue\n    rows.append({\n        \"video_path\": str(vp),\n        \"label_name\": lbl,\n        \"label_id\": LABEL2ID[lbl],\n        \"video_id\": vp.stem\n    })\n\ndf_videos = pd.DataFrame(rows)\ndisplay(df_videos.head())\nprint(\"N videos:\", len(df_videos))\nprint(df_videos[\"label_name\"].value_counts())\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3. OpenPose extraction → `.npz` (optional in runtime)\n\n**Owners:** Person A (primary), Person B (review)\n\n> If OpenPose is not available in the grading environment, keep these cells for documentation and run them locally.  \n> The notebook should still run starting from the `.npz` loading section.\n\n### TODO\n- [ ] Decide extraction approach:\n  - [ ] OpenPose CLI (recommended)\n  - [ ] Python API bindings\n- [ ] For each video:\n  - [ ] Run OpenPose with BODY-25\n  - [ ] Parse per-frame JSON to `[T, 25, 3]` (x,y,conf)\n  - [ ] Normalize pose (`pose_norm`)\n  - [ ] Save `.npz` with required fields\n\n### Notes\n- Handle missing detections (conf=0) robustly.\n- Keep frame indices to support debugging and plotting.\n"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# OPTIONAL: OpenPose runner (CLI-based template)\n",
    "\n",
    "import subprocess\n",
    "\n",
    "def run_openpose_cli(video_path: Path, out_json_dir: Path, openpose_bin: Path, model=\"BODY_25\"):\n",
    "    \"\"\"Run OpenPose on a single video and save per-frame JSON outputs.\"\"\"\n",
    "    out_json_dir.mkdir(parents=True, exist_ok=True)\n",
    "    # TODO (Person A): adjust flags to your OpenPose install\n",
    "    cmd = [\n",
    "        str(openpose_bin),\n",
    "        \"--video\", str(video_path),\n",
    "        \"--write_json\", str(out_json_dir),\n",
    "        \"--display\", \"0\",\n",
    "        \"--render_pose\", \"0\",\n",
    "        \"--model_pose\", model,\n",
    "    ]\n",
    "    # print(\" \".join(cmd))\n",
    "    subprocess.run(cmd, check=True)\n",
    "\n",
    "def parse_openpose_json_sequence(json_dir: Path):\n",
    "    \"\"\"Parse OpenPose JSON files into array [T, 25, 3] => x,y,conf.\"\"\"\n",
    "    json_files = sorted(json_dir.glob(\"*.json\"))\n",
    "    seq = []\n",
    "    frames = []\n",
    "    for jf in json_files:\n",
    "        with open(jf, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "        people = data.get(\"people\", [])\n",
    "        if not people:\n",
    "            keypoints = np.zeros((25, 3), dtype=np.float32)\n",
    "        else:\n",
    "            # Pick the person with the highest total confidence\n",
    "            best_kp = None\n",
    "            best_score = -1.0\n",
    "            for person in people:\n",
    "                kp = person.get(\"pose_keypoints_2d\", [])\n",
    "                if not kp:\n",
    "                    continue\n",
    "                kp = np.array(kp, dtype=np.float32).reshape(-1, 3)\n",
    "                score = float(kp[:, 2].sum())\n",
    "                if score > best_score:\n",
    "                    best_score = score\n",
    "                    best_kp = kp\n",
    "            if best_kp is None:\n",
    "                keypoints = np.zeros((25, 3), dtype=np.float32)\n",
    "            else:\n",
    "                keypoints = best_kp[:25]\n",
    "        seq.append(keypoints)\n",
    "        frames.append(len(frames))\n",
    "    seq = np.stack(seq, axis=0) if len(seq) else np.zeros((0,25,3), dtype=np.float32)\n",
    "    frames = np.array(frames, dtype=np.int32)\n",
    "    return seq, frames\n",
    "\n",
    "def normalize_pose(seq: np.ndarray):\n",
    "    \"\"\"Normalize pose in a simple, defensible way. Replace with your method.\"\"\"\n",
    "    # seq: [T,25,3]\n",
    "    # TODO (Person A): implement normalization (center + scale)\n",
    "    seq_norm = seq.copy()\n",
    "    # Example: center x,y by per-frame mean of valid joints\n",
    "    xy = seq_norm[..., :2]\n",
    "    conf = seq_norm[..., 2:3]\n",
    "    valid = (conf > 0).astype(np.float32)\n",
    "    denom = np.maximum(valid.sum(axis=1, keepdims=True), 1.0)\n",
    "    center = (xy * valid).sum(axis=1, keepdims=True) / denom\n",
    "    xy = xy - center\n",
    "    seq_norm[..., :2] = xy\n",
    "    return seq_norm\n",
    "\n",
    "def build_npz_for_video(video_row, npz_root: Path):\n",
    "    vp = Path(video_row[\"video_path\"])\n",
    "    label_id = int(video_row[\"label_id\"])\n",
    "    label_name = video_row[\"label_name\"]\n",
    "    out_path = npz_root / f\"{vp.stem}.npz\"\n",
    "    if out_path.exists():\n",
    "        return str(out_path)\n",
    "\n",
    "    tmp_json_dir = npz_root / \"_openpose_json\" / vp.stem\n",
    "\n",
    "    # Option 1: run OpenPose now (uncomment when ready)\n",
    "    # run_openpose_cli(vp, tmp_json_dir, OPENPOSE_BIN)\n",
    "\n",
    "    if not tmp_json_dir.exists():\n",
    "        raise RuntimeError(\n",
    "            \"OpenPose JSON not found. Run OpenPose or point to existing output in tmp_json_dir.\"\n",
    "        )\n",
    "\n",
    "    seq, frames = parse_openpose_json_sequence(tmp_json_dir)\n",
    "    if seq.shape[0] == 0:\n",
    "        raise RuntimeError(f\"No frames parsed for {vp}\")\n",
    "\n",
    "    seq_norm = normalize_pose(seq)\n",
    "\n",
    "    npz_root.mkdir(parents=True, exist_ok=True)\n",
    "    np.savez_compressed(\n",
    "        out_path,\n",
    "        pose=seq.astype(np.float32),\n",
    "        pose_norm=seq_norm.astype(np.float32),\n",
    "        frames=frames,\n",
    "        label=label_id,\n",
    "        label_name=label_name,\n",
    "        video_path=str(vp),\n",
    "    )\n",
    "    return str(out_path)\n",
    "\n",
    "# Batch template (run locally)\n",
    "# for _, row in tqdm(df_videos.iterrows(), total=len(df_videos)):\n",
    "#     build_npz_for_video(row, NPZ_ROOT)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4. Load `.npz` pose dataset → unified in-memory samples\n\n**Owners:** Person A (primary), Person B (review)\n\n### TODO\n- [ ] Implement loader that reads:\n  - `pose_norm` (preferred) or `pose`\n  - `label`\n  - `frames` (optional)\n- [ ] Convert each sample to a standard representation:\n  - `X_i` as `[T, D]` where `D = 25*2` or `25*3`\n- [ ] Create `samples` list with fields:\n  - `X`, `y`, `length`, `video_id`\n- [ ] Summarize lengths and class counts\n"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "def load_npz_samples(npz_root: Path):\n    npz_files = sorted(npz_root.glob(\"*.npz\"))\n    samples = []\n    for f in npz_files:\n        data = np.load(f, allow_pickle=True)\n        pose = data[\"pose_norm\"] if \"pose_norm\" in data.files else data[\"pose\"]  # [T,25,3]\n        y = int(data[\"label\"])\n        video_id = Path(str(data.get(\"video_path\", f.stem))).stem\n\n        # Joint subset\n        pose = pose[:, JOINT_IDS, :] if USE_JOINT_SUBSET else pose[:, :25, :]\n\n        # Feature dims\n        if USE_CONFIDENCE:\n            feat = pose.reshape(pose.shape[0], -1)            # [T, 25*3]\n        else:\n            feat = pose[..., :2].reshape(pose.shape[0], -1)   # [T, 25*2]\n\n        samples.append({\n            \"X\": feat.astype(np.float32),\n            \"y\": y,\n            \"length\": int(feat.shape[0]),\n            \"video_id\": video_id,\n            \"npz_path\": str(f),\n        })\n    return samples\n\nsamples = load_npz_samples(NPZ_ROOT)\nprint(\"Loaded samples:\", len(samples))\nprint(\"Class counts:\", pd.Series([s['y'] for s in samples]).value_counts().sort_index().to_dict())\nlengths = np.array([s[\"length\"] for s in samples])\nprint(\"Length stats:\", dict(min=int(lengths.min()), max=int(lengths.max()), mean=float(lengths.mean()), median=float(np.median(lengths))))\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 5. Train/test split + shared preprocessing utilities\n\n**Owners:** Person B (primary), Person A (review)\n\n### TODO\n- [ ] Implement stratified split\n- [ ] Decide scaling:\n  - [ ] per-sequence scaler (safe, no leakage)\n  - [ ] global scaler fitted on train only (be careful)\n- [ ] Decide fixed-length policy for neural models:\n  - [ ] `pad/truncate to T_MAX`\n  - [ ] or keep variable-length with packing (recommended for LSTM)\n- [ ] Implement helper functions reused by all methods\n"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "def stratified_split(samples, test_size=0.2, seed=SEED):\n    y = np.array([s[\"y\"] for s in samples])\n    idx = np.arange(len(samples))\n    tr_idx, te_idx = train_test_split(idx, test_size=test_size, random_state=seed, stratify=y)\n    train_samples = [samples[i] for i in tr_idx]\n    test_samples  = [samples[i] for i in te_idx]\n    return train_samples, test_samples\n\ntrain_samples, test_samples = stratified_split(samples, test_size=0.2)\nprint(\"Train:\", len(train_samples), \"Test:\", len(test_samples))\n\ndef pad_or_truncate(X: np.ndarray, T_max=T_MAX):\n    T, D = X.shape\n    if T == T_max:\n        return X\n    if T > T_max:\n        return X[:T_max]\n    pad = np.zeros((T_max - T, D), dtype=X.dtype)\n    return np.vstack([X, pad])\n\ndef to_tslearn_dataset(sample_list):\n    # tslearn wants array-like of shape [N, T, D] (possibly ragged -> to_time_series_dataset)\n    X = [s[\"X\"] for s in sample_list]\n    return to_time_series_dataset(X)\n\ndef get_xy(sample_list):\n    X = [s[\"X\"] for s in sample_list]\n    y = np.array([s[\"y\"] for s in sample_list], dtype=np.int64)\n    return X, y\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 6. Evaluation helpers (metrics, confusion matrices, result logging)\n\n**Owners:** Person B (primary), Person A (review)\n\n### TODO\n- [ ] Implement a standard evaluation dictionary (accuracy, macro-F1, per-class report)\n- [ ] Implement confusion matrix plotting + saving\n- [ ] Create a `results` list of dicts for experiment tracking\n"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "def evaluate_and_report(y_true, y_pred, title=\"\", save_cm_path=None):\n    acc = accuracy_score(y_true, y_pred)\n    f1  = f1_score(y_true, y_pred, average=\"macro\")\n    print(f\"{title}  |  acc={acc:.4f}  macroF1={f1:.4f}\")\n    print(classification_report(y_true, y_pred, target_names=CLASS_NAMES, digits=4))\n\n    cm = confusion_matrix(y_true, y_pred, labels=list(range(len(CLASS_NAMES))))\n    disp = ConfusionMatrixDisplay(cm, display_labels=CLASS_NAMES)\n    fig, ax = plt.subplots(figsize=(7,6))\n    disp.plot(ax=ax, cmap=\"Blues\", colorbar=False, xticks_rotation=45)\n    ax.set_title(title)\n    plt.tight_layout()\n    if save_cm_path is not None:\n        fig.savefig(save_cm_path, dpi=200)\n    plt.show()\n\n    return {\"title\": title, \"accuracy\": acc, \"macro_f1\": f1, \"cm\": cm}\n\nRESULTS = []\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# METHOD 1 — GAK + SVM (required)\n\n**Owners:** Person B (primary), Person A (review)\n\n### Checklist\n- [ ] Prepare sequences for tslearn (`[N, T, D]`, variable length allowed)\n- [ ] Compute GAK Gram matrix for train (`K_train`)\n- [ ] Train SVM on precomputed kernel\n- [ ] Compute `K_test` and predict\n- [ ] Log metrics + save confusion matrix\n- [ ] Ablation study:\n  - [ ] `sigma` (kernel bandwidth) sweep\n  - [ ] SVM `C` sweep\n  - [ ] optional: joint subset / confidence feature toggle\n"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "def gak_svm_train_predict(train_samples, test_samples, sigma=1.0, C=1.0):\n    # Prepare\n    X_train, y_train = get_xy(train_samples)\n    X_test,  y_test  = get_xy(test_samples)\n\n    Xtr = to_time_series_dataset(X_train)  # [N, T, D] with padding by tslearn\n    Xte = to_time_series_dataset(X_test)\n\n    # Optional scaling (per-series)\n    scaler = TimeSeriesScalerMeanVariance()\n    Xtr_s = scaler.fit_transform(Xtr)\n    Xte_s = scaler.transform(Xte)\n\n    # GAK Gram matrices\n    K_train = cdist_gak(Xtr_s, Xtr_s, sigma=sigma)\n    K_test  = cdist_gak(Xte_s, Xtr_s, sigma=sigma)\n\n    clf = SVC(kernel=\"precomputed\", C=C)\n    clf.fit(K_train, y_train)\n    y_pred = clf.predict(K_test)\n    return y_test, y_pred\n\n# Quick single run (edit params)\n# y_true, y_pred = gak_svm_train_predict(train_samples, test_samples, sigma=1.0, C=1.0)\n# metrics = evaluate_and_report(y_true, y_pred, title=\"GAK+SVM\", save_cm_path=FIG_DIR/\"cm_gak_svm.png\")\n# RESULTS.append({\"method\":\"GAK+SVM\", \"sigma\":1.0, \"C\":1.0, **metrics})\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## METHOD 1 — Ablation grid (GAK+SVM)\n\n**Owners:** Person B\n\n### TODO\n- [ ] Define a reasonable grid (e.g., 6–12 runs total)\n- [ ] Save results to CSV\n- [ ] Plot performance vs sigma and vs C\n"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "def run_gak_svm_ablation(train_samples, test_samples, sigmas, Cs):\n    for sigma in sigmas:\n        for C in Cs:\n            y_true, y_pred = gak_svm_train_predict(train_samples, test_samples, sigma=sigma, C=C)\n            title = f\"GAK+SVM sigma={sigma} C={C}\"\n            cm_path = FIG_DIR / f\"cm_gak_svm_sigma{sigma}_C{C}.png\"\n            metrics = evaluate_and_report(y_true, y_pred, title=title, save_cm_path=cm_path)\n            RESULTS.append({\"method\":\"GAK+SVM\", \"sigma\":sigma, \"C\":C, **metrics})\n\n# Example grid (edit)\n# sigmas = [0.5, 1.0, 2.0]\n# Cs = [0.1, 1.0, 10.0]\n# run_gak_svm_ablation(train_samples, test_samples, sigmas, Cs)\n\n# Save table\n# pd.DataFrame(RESULTS).to_csv(RES_DIR/\"results_all.csv\", index=False)\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# METHOD 2 — Shapelets + MLP (required)\n\n**Owners:** Person A (primary for shapelets), Person B (primary for PyTorch MLP)\n\n### Checklist\n- [ ] Fit shapelet transform/model on training set\n- [ ] Transform train/test to fixed-length feature vectors\n- [ ] Train MLP classifier (PyTorch)\n- [ ] Evaluate + confusion matrix\n- [ ] Ablations:\n  - [ ] shapelet sizes / counts\n  - [ ] MLP hidden size / depth / dropout\n  - [ ] learning rate / epochs\n"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Shapelets imports (may fail if tslearn version differs)\n",
    "try:\n",
    "    from tslearn.shapelets import ShapeletModel\n",
    "except Exception as e:\n",
    "    print(\"ShapeletModel import issue:\", e)\n",
    "\n",
    "def build_shapelet_datasets(train_samples, test_samples):\n",
    "    # For shapelets, use [N,T,D] with possible scaling\n",
    "    X_train, y_train = get_xy(train_samples)\n",
    "    X_test,  y_test  = get_xy(test_samples)\n",
    "    Xtr = to_time_series_dataset(X_train)\n",
    "    Xte = to_time_series_dataset(X_test)\n",
    "    scaler = TimeSeriesScalerMeanVariance()\n",
    "    Xtr_s = scaler.fit_transform(Xtr)\n",
    "    Xte_s = scaler.transform(Xte)\n",
    "    return Xtr_s, y_train, Xte_s, y_test\n",
    "\n",
    "def fit_shapelets(Xtr, ytr, n_shapelets_per_size, max_iter=50):\n",
    "    \"\"\"Fit shapelet model. You must tune sizes/counts.\"\"\"\n",
    "    # TODO (Person A): choose params based on assignment + ablations\n",
    "    shp = ShapeletModel(\n",
    "        n_shapelets_per_size=n_shapelets_per_size,\n",
    "        optimizer=\"adam\",\n",
    "        weight_regularizer=0.01,\n",
    "        max_iter=max_iter,\n",
    "        verbose=1,\n",
    "        random_state=SEED\n",
    "    )\n",
    "    shp.fit(Xtr, ytr)\n",
    "    return shp\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, num_classes, dropout=0.2):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        d = input_dim\n",
    "        for h in hidden_dims:\n",
    "            layers += [nn.Linear(d, h), nn.ReLU(), nn.Dropout(dropout)]\n",
    "            d = h\n",
    "        layers += [nn.Linear(d, num_classes)]\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "def train_mlp_on_features(Xtr_feat, ytr, Xte_feat, yte, hidden_dims=(256,128), lr=1e-3, epochs=30, batch_size=64, dropout=0.2):\n",
    "    Xtr_t = torch.tensor(Xtr_feat, dtype=torch.float32)\n",
    "    ytr_t = torch.tensor(ytr, dtype=torch.long)\n",
    "    Xte_t = torch.tensor(Xte_feat, dtype=torch.float32)\n",
    "    yte_t = torch.tensor(yte, dtype=torch.long)\n",
    "\n",
    "    train_ds = torch.utils.data.TensorDataset(Xtr_t, ytr_t)\n",
    "    test_ds  = torch.utils.data.TensorDataset(Xte_t, yte_t)\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "    test_loader  = DataLoader(test_ds, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    model = MLP(Xtr_feat.shape[1], list(hidden_dims), len(CLASS_NAMES), dropout=dropout).to(DEVICE)\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    history = {\"train_loss\":[], \"train_acc\":[], \"test_acc\":[]}\n",
    "\n",
    "    for ep in range(1, epochs+1):\n",
    "        model.train()\n",
    "        total_loss=0.0\n",
    "        correct=0\n",
    "        n=0\n",
    "        for xb, yb in train_loader:\n",
    "            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "            opt.zero_grad()\n",
    "            logits = model(xb)\n",
    "            loss = loss_fn(logits, yb)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            total_loss += loss.item()*len(xb)\n",
    "            pred = logits.argmax(dim=1)\n",
    "            correct += (pred==yb).sum().item()\n",
    "            n += len(xb)\n",
    "\n",
    "        train_loss = total_loss/n\n",
    "        train_acc = correct/n\n",
    "\n",
    "        model.eval()\n",
    "        all_pred=[]\n",
    "        all_true=[]\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in test_loader:\n",
    "                xb = xb.to(DEVICE)\n",
    "                logits = model(xb)\n",
    "                pred = logits.argmax(dim=1).cpu().numpy()\n",
    "                all_pred.append(pred)\n",
    "                all_true.append(yb.numpy())\n",
    "        all_pred = np.concatenate(all_pred)\n",
    "        all_true = np.concatenate(all_true)\n",
    "        test_acc = accuracy_score(all_true, all_pred)\n",
    "\n",
    "        history[\"train_loss\"].append(train_loss)\n",
    "        history[\"train_acc\"].append(train_acc)\n",
    "        history[\"test_acc\"].append(test_acc)\n",
    "\n",
    "        if ep % max(1, epochs//10) == 0 or ep == 1:\n",
    "            print(f\"epoch {ep:03d} | loss {train_loss:.4f} | train_acc {train_acc:.4f} | test_acc {test_acc:.4f}\")\n",
    "\n",
    "    return model, history, all_true, all_pred\n",
    "\n",
    "# Pipeline template (uncomment after implementing shapelet transform properly)\n",
    "# Xtr_s, ytr, Xte_s, yte = build_shapelet_datasets(train_samples, test_samples)\n",
    "# shapelet_model = fit_shapelets(Xtr_s, ytr, n_shapelets_per_size={10:5, 20:5}, max_iter=50)\n",
    "# Xtr_feat = shapelet_model.transform(Xtr_s)\n",
    "# Xte_feat = shapelet_model.transform(Xte_s)\n",
    "# mlp_model, hist, y_true, y_pred = train_mlp_on_features(Xtr_feat, ytr, Xte_feat, yte)\n",
    "# metrics = evaluate_and_report(y_true, y_pred, title=\"Shapelets+MLP\", save_cm_path=FIG_DIR/\"cm_shapelets_mlp.png\")\n",
    "# RESULTS.append({\"method\":\"Shapelets+MLP\", **metrics})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## METHOD 2 — Ablation grid (Shapelets+MLP)\n\n**Owners:** Person A (shapelet config sweep), Person B (MLP sweep)\n\n### TODO\n- [ ] Define shapelet configs:\n  - [ ] sizes (e.g., 10/20/30)\n  - [ ] counts per size (small/med/large)\n  - [ ] max_iter\n- [ ] Define MLP configs:\n  - [ ] hidden_dims, dropout\n  - [ ] lr, epochs\n- [ ] Run a controlled grid (aim ~8–16 runs total) and log results\n"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "# TODO: Implement ablation runner similar to GAK+SVM.\n# Suggestions:\n# - Fix MLP config and sweep shapelets\n# - Then fix best shapelets and sweep MLP hyperparams\n#\n# Save:\n# pd.DataFrame(RESULTS).to_csv(RES_DIR/\"results_all.csv\", index=False)\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# METHOD 3 — LSTM classifier (required)\n\n**Owners:** Person B (primary), Person A (review)\n\n### Checklist\n- [ ] Build PyTorch Dataset that returns variable-length sequences\n- [ ] Implement padded collate + lengths\n- [ ] Implement LSTM with packing (`pack_padded_sequence`)\n- [ ] Train, evaluate, log\n- [ ] Ablations:\n  - [ ] hidden size, layers\n  - [ ] bidirectional on/off\n  - [ ] dropout\n  - [ ] confidence on/off, joint subset\n"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "class PoseSeqDataset(Dataset):\n    def __init__(self, sample_list, pad_to_maxlen=False, T_max=T_MAX):\n        self.samples = sample_list\n        self.pad_to_maxlen = pad_to_maxlen\n        self.T_max = T_max\n\n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, idx):\n        s = self.samples[idx]\n        X = s[\"X\"]\n        y = s[\"y\"]\n        if self.pad_to_maxlen:\n            X = pad_or_truncate(X, self.T_max)\n            length = min(s[\"length\"], self.T_max)\n        else:\n            length = s[\"length\"]\n        return torch.tensor(X, dtype=torch.float32), torch.tensor(y, dtype=torch.long), length\n\ndef collate_pad(batch):\n    xs, ys, lens = zip(*batch)\n    lens = torch.tensor(lens, dtype=torch.long)\n    xs_padded = pad_sequence(xs, batch_first=True)  # [B, T_max, D]\n    ys = torch.stack(ys)\n    return xs_padded, lens, ys\n\nclass LSTMClassifier(nn.Module):\n    def __init__(self, input_dim, hidden_size, num_layers, num_classes, bidirectional=False, dropout=0.2):\n        super().__init__()\n        self.bidirectional = bidirectional\n        self.lstm = nn.LSTM(\n            input_dim, hidden_size, num_layers=num_layers,\n            batch_first=True,\n            bidirectional=bidirectional,\n            dropout=dropout if num_layers > 1 else 0.0\n        )\n        out_dim = hidden_size * (2 if bidirectional else 1)\n        self.fc = nn.Linear(out_dim, num_classes)\n\n    def forward(self, x, lengths):\n        # x: [B,T,D], lengths: [B]\n        lengths_sorted, idx_sort = torch.sort(lengths, descending=True)\n        x_sorted = x[idx_sort]\n\n        packed = pack_padded_sequence(x_sorted, lengths_sorted.cpu(), batch_first=True, enforce_sorted=True)\n        packed_out, (hn, cn) = self.lstm(packed)\n\n        # last layer hidden\n        if self.bidirectional:\n            # hn: [num_layers*2, B, H] -> take last layer forward/back\n            forward_last = hn[-2]\n            backward_last = hn[-1]\n            h_last = torch.cat([forward_last, backward_last], dim=1)\n        else:\n            h_last = hn[-1]  # [B,H]\n\n        # unsort\n        _, idx_unsort = torch.sort(idx_sort)\n        h_last = h_last[idx_unsort]\n\n        logits = self.fc(h_last)\n        return logits\n\ndef train_lstm(train_samples, test_samples, hidden_size=128, num_layers=2, bidirectional=True, dropout=0.2, lr=1e-3, epochs=25, batch_size=32):\n    train_ds = PoseSeqDataset(train_samples, pad_to_maxlen=False)\n    test_ds  = PoseSeqDataset(test_samples,  pad_to_maxlen=False)\n\n    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, collate_fn=collate_pad)\n    test_loader  = DataLoader(test_ds,  batch_size=batch_size, shuffle=False, collate_fn=collate_pad)\n\n    input_dim = train_samples[0][\"X\"].shape[1]\n    model = LSTMClassifier(input_dim, hidden_size, num_layers, len(CLASS_NAMES), bidirectional=bidirectional, dropout=dropout).to(DEVICE)\n\n    opt = torch.optim.Adam(model.parameters(), lr=lr)\n    loss_fn = nn.CrossEntropyLoss()\n\n    history = {\"train_loss\":[], \"train_acc\":[], \"test_acc\":[]}\n\n    for ep in range(1, epochs+1):\n        model.train()\n        total_loss=0.0\n        correct=0\n        n=0\n        for xb, lens, yb in train_loader:\n            xb, lens, yb = xb.to(DEVICE), lens.to(DEVICE), yb.to(DEVICE)\n            opt.zero_grad()\n            logits = model(xb, lens)\n            loss = loss_fn(logits, yb)\n            loss.backward()\n            opt.step()\n            total_loss += loss.item()*len(xb)\n            pred = logits.argmax(dim=1)\n            correct += (pred==yb).sum().item()\n            n += len(xb)\n        train_loss = total_loss/n\n        train_acc = correct/n\n\n        model.eval()\n        all_pred=[]\n        all_true=[]\n        with torch.no_grad():\n            for xb, lens, yb in test_loader:\n                xb, lens = xb.to(DEVICE), lens.to(DEVICE)\n                logits = model(xb, lens)\n                pred = logits.argmax(dim=1).cpu().numpy()\n                all_pred.append(pred)\n                all_true.append(yb.numpy())\n        all_pred = np.concatenate(all_pred)\n        all_true = np.concatenate(all_true)\n        test_acc = accuracy_score(all_true, all_pred)\n\n        history[\"train_loss\"].append(train_loss)\n        history[\"train_acc\"].append(train_acc)\n        history[\"test_acc\"].append(test_acc)\n\n        if ep % max(1, epochs//10) == 0 or ep == 1:\n            print(f\"epoch {ep:03d} | loss {train_loss:.4f} | train_acc {train_acc:.4f} | test_acc {test_acc:.4f}\")\n\n    return model, history, all_true, all_pred\n\n# Run template\n# lstm_model, lstm_hist, y_true, y_pred = train_lstm(train_samples, test_samples)\n# metrics = evaluate_and_report(y_true, y_pred, title=\"LSTM\", save_cm_path=FIG_DIR/\"cm_lstm.png\")\n# RESULTS.append({\"method\":\"LSTM\", **metrics})\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## METHOD 3 — Ablation grid (LSTM)\n\n**Owners:** Person B\n\n### TODO\n- [ ] Sweep:\n  - [ ] hidden_size: 64/128/256\n  - [ ] num_layers: 1/2/3\n  - [ ] bidirectional: False/True\n  - [ ] dropout: 0.0/0.2/0.5\n- [ ] Log results + pick best\n"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "# TODO: implement ablation runner\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# METHOD 4 — Extra time-series classifier (required: choose one)\n\n**Owners:** Person A (primary), Person B (review)\n\nRecommended: **Temporal 1D CNN** (strong baseline, easy ablations)\n\n### Checklist\n- [ ] Implement TemporalCNN classifier on pose sequences\n- [ ] Decide how to handle variable length:\n  - [ ] fixed-length pad/truncate to T_MAX, or\n  - [ ] global pooling over time with masking\n- [ ] Train + evaluate + log\n- [ ] Ablations:\n  - [ ] kernel sizes (3/5/7)\n  - [ ] channels/blocks\n  - [ ] pooling type\n"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "class TemporalCNN(nn.Module):\n    def __init__(self, input_dim, num_classes, channels=(128,128), kernel_size=5, dropout=0.2):\n        super().__init__()\n        layers = []\n        in_ch = input_dim\n        for ch in channels:\n            layers += [\n                nn.Conv1d(in_ch, ch, kernel_size=kernel_size, padding=kernel_size//2),\n                nn.ReLU(),\n                nn.Dropout(dropout),\n                nn.MaxPool1d(kernel_size=2),\n            ]\n            in_ch = ch\n        self.conv = nn.Sequential(*layers)\n        self.head = nn.Sequential(\n            nn.AdaptiveAvgPool1d(1),\n            nn.Flatten(),\n            nn.Linear(in_ch, num_classes)\n        )\n\n    def forward(self, x):\n        # x: [B,T,D] -> [B,D,T]\n        x = x.transpose(1,2)\n        z = self.conv(x)\n        return self.head(z)\n\ndef train_temporal_cnn(train_samples, test_samples, channels=(128,128), kernel_size=5, dropout=0.2, lr=1e-3, epochs=25, batch_size=32):\n    # Fixed-length for CNN\n    train_ds = PoseSeqDataset(train_samples, pad_to_maxlen=True, T_max=T_MAX)\n    test_ds  = PoseSeqDataset(test_samples,  pad_to_maxlen=True, T_max=T_MAX)\n\n    def collate_fixed(batch):\n        xs, ys, lens = zip(*batch)\n        xs = torch.stack(xs)  # already fixed [T_MAX,D]\n        ys = torch.stack(ys)\n        return xs, ys\n\n    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, collate_fn=collate_fixed)\n    test_loader  = DataLoader(test_ds,  batch_size=batch_size, shuffle=False, collate_fn=collate_fixed)\n\n    input_dim = train_samples[0][\"X\"].shape[1]\n    model = TemporalCNN(input_dim, len(CLASS_NAMES), channels=channels, kernel_size=kernel_size, dropout=dropout).to(DEVICE)\n    opt = torch.optim.Adam(model.parameters(), lr=lr)\n    loss_fn = nn.CrossEntropyLoss()\n\n    history = {\"train_loss\":[], \"train_acc\":[], \"test_acc\":[]}\n\n    for ep in range(1, epochs+1):\n        model.train()\n        total_loss=0.0\n        correct=0\n        n=0\n        for xb, yb in train_loader:\n            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n            opt.zero_grad()\n            logits = model(xb)\n            loss = loss_fn(logits, yb)\n            loss.backward()\n            opt.step()\n            total_loss += loss.item()*len(xb)\n            pred = logits.argmax(dim=1)\n            correct += (pred==yb).sum().item()\n            n += len(xb)\n        train_loss = total_loss/n\n        train_acc = correct/n\n\n        model.eval()\n        all_pred=[]\n        all_true=[]\n        with torch.no_grad():\n            for xb, yb in test_loader:\n                xb = xb.to(DEVICE)\n                logits = model(xb)\n                pred = logits.argmax(dim=1).cpu().numpy()\n                all_pred.append(pred)\n                all_true.append(yb.numpy())\n        all_pred = np.concatenate(all_pred)\n        all_true = np.concatenate(all_true)\n        test_acc = accuracy_score(all_true, all_pred)\n\n        history[\"train_loss\"].append(train_loss)\n        history[\"train_acc\"].append(train_acc)\n        history[\"test_acc\"].append(test_acc)\n\n        if ep % max(1, epochs//10) == 0 or ep == 1:\n            print(f\"epoch {ep:03d} | loss {train_loss:.4f} | train_acc {train_acc:.4f} | test_acc {test_acc:.4f}\")\n\n    return model, history, all_true, all_pred\n\n# Run template\n# cnn_model, cnn_hist, y_true, y_pred = train_temporal_cnn(train_samples, test_samples)\n# metrics = evaluate_and_report(y_true, y_pred, title=\"TemporalCNN\", save_cm_path=FIG_DIR/\"cm_temporalcnn.png\")\n# RESULTS.append({\"method\":\"TemporalCNN\", **metrics})\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## METHOD 4 — Ablation grid (Extra method)\n\n**Owners:** Person A\n\n### TODO\n- [ ] Sweep:\n  - [ ] kernel_size: 3/5/7\n  - [ ] channels: (64,64) vs (128,128) vs (256,256)\n  - [ ] dropout: 0.0/0.2/0.5\n- [ ] Log results, pick best\n"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "# TODO: implement ablation runner\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# 7. Results aggregation & comparison (ALL methods)\n\n**Owners:** Person A + Person B\n\n### Checklist\n- [ ] Convert `RESULTS` to DataFrame\n- [ ] Save to CSV in `outputs/results/`\n- [ ] Create a summary table (best per method)\n- [ ] Create at least:\n  - [ ] overall comparison bar plot (accuracy or macro-F1)\n  - [ ] ablation plots (e.g., performance vs hyperparameter)\n- [ ] Write short analysis paragraphs:\n  - [ ] Which method won and why?\n  - [ ] What hyperparameters mattered most?\n  - [ ] Which classes were confused most and why (based on CM)?\n"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "def results_to_df(results_list):\n    # Drop huge matrices for CSV friendliness\n    rows=[]\n    for r in results_list:\n        rr = dict(r)\n        if \"cm\" in rr:\n            rr[\"cm\"] = rr[\"cm\"].tolist()  # still ok; or remove\n        rows.append(rr)\n    return pd.DataFrame(rows)\n\n# df_results = results_to_df(RESULTS)\n# display(df_results.head())\n# df_results.to_csv(RES_DIR/\"results_all.csv\", index=False)\n\n# Best-per-method summary\n# if len(df_results):\n#     best = df_results.sort_values([\"method\",\"macro_f1\"], ascending=[True, False]).groupby(\"method\").head(1)\n#     display(best[[\"method\",\"accuracy\",\"macro_f1\",\"title\"]])\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Write-up (final report sections inside notebook)\n",
    "\n",
    "**Owners:** Person A + Person B\n",
    "\n",
    "### Required narrative (keep it concise but concrete, self-contained)\n",
    "- [ ] Dataset overview & pose extraction\n",
    "- [ ] Preprocessing choices (normalization, missing joints, padding)\n",
    "- [ ] Include pseudocode or figures where they clarify key steps\n",
    "- [ ] Summarize ablation design (what you swept, why, and what changed)\n",
    "- [ ] For each method:\n",
    "  - [ ] brief description\n",
    "  - [ ] hyperparameter search / ablation design\n",
    "  - [ ] best settings and results\n",
    "  - [ ] confusion matrix interpretation\n",
    "- [ ] Cross-method comparison and conclusion\n",
    "- [ ] Limitations and future work\n",
    "\n",
    "> Tip: Write like a scientist, not like a poet—save poetry for your Git commit messages.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 9. Appendix (optional)\n\n**Owners:** whoever has energy left\n\n### Ideas\n- [ ] Visualize sample skeletons over time (animated or key frames)\n- [ ] Plot joint trajectories for each class\n- [ ] Class imbalance handling tests\n- [ ] Sensitivity to T_MAX or resampling rate\n"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}